#!/usr/bin/env python3
# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
################################################################################

"""
ðŸš€ Rapid Fuzzer Expansion Script
The most comprehensive fuzzing expansion system ever created!

Features:
  â€¢ ðŸŽ­ Spectacular visual feedback and progress tracking
  â€¢ ðŸ§  Advanced analytics with pattern-based recommendations
  â€¢ âš¡ Performance optimization and parallel processing
  â€¢ ðŸŽ¯ Multi-language support with intelligent detection
  â€¢ ðŸ“Š Real-time monitoring and live statistics
  â€¢ ðŸ”§ Auto-optimization and smart caching
  â€¢ ðŸŽ¨ Beautiful reporting with charts and visualizations

Integrates Python helper with enhanced build script for comprehensive fuzzing expansion
"""

import os
import sys
import subprocess
import argparse
import json
import time
import threading
import logging
import re
from pathlib import Path
from datetime import datetime
import tempfile
import shutil
import hashlib
from typing import Dict, List, Optional, Union, Any, Tuple
import concurrent.futures
from dataclasses import dataclass, asdict, field
from contextlib import contextmanager
import functools
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Optional dependencies - graceful fallback if not available
try:
    import psutil  # type: ignore
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    # Create a mock psutil for type hints
    class MockPsutil:
        class Process:
            def memory_info(self):
                class MemoryInfo:
                    rss = 0
                return MemoryInfo()
        def cpu_percent(self, interval=0):
            return 0.0
        def virtual_memory(self):
            class VirtualMemory:
                percent = 0.0
            return VirtualMemory()
    psutil = MockPsutil()

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # Create a mock tqdm for type hints
    class MockTqdm:
        def __init__(self, *args, **kwargs):
            self.total = kwargs.get('total', 0)
            self.current = 0
        def update(self, n=1):
            self.current += n
        def close(self):
            pass
    tqdm = MockTqdm

# Phase 6: The Pattern-Based Core - Self-Improving Fuzzing Platform
import sqlite3
import hashlib
import json
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple
import threading
import atexit

class AnalyticsDatabase:
    """Persistent analytics database for the sentient core"""

    def __init__(self, db_path: str = "analytics.db"):
        self.db_path = db_path
        self._local = threading.local()
        self._init_database()
        atexit.register(self.close_all_connections)

    @property
    def connection(self):
        if not hasattr(self._local, 'conn') or self._local.conn is None:
            self._local.conn = sqlite3.connect(self.db_path)
            self._local.conn.execute("PRAGMA journal_mode=WAL")
            self._local.conn.execute("PRAGMA synchronous=NORMAL")
        return self._local.conn

    def _init_database(self):
        """Initialize database schema"""
        conn = self.connection
        cursor = conn.cursor()

        # Run history table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS run_history (
                run_id TEXT PRIMARY KEY,
                timestamp REAL NOT NULL,
                source_directory TEXT NOT NULL,
                total_files INTEGER NOT NULL,
                processing_time REAL NOT NULL,
                success_rate REAL NOT NULL,
                config_json TEXT NOT NULL
            )
        ''')

        # Language detection patterns
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS language_patterns (
                pattern_id TEXT PRIMARY KEY,
                language TEXT NOT NULL,
                pattern_type TEXT NOT NULL,
                pattern_regex TEXT NOT NULL,
                success_count INTEGER DEFAULT 0,
                failure_count INTEGER DEFAULT 0,
                avg_confidence REAL DEFAULT 0.0,
                last_used REAL DEFAULT 0,
                created_at REAL DEFAULT (strftime('%s', 'now'))
            )
        ''')

        # Dictionary keywords and their effectiveness
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS keyword_effectiveness (
                keyword_id TEXT PRIMARY KEY,
                keyword TEXT NOT NULL,
                language TEXT NOT NULL,
                source_type TEXT NOT NULL,
                success_count INTEGER DEFAULT 0,
                coverage_improvement REAL DEFAULT 0.0,
                last_success REAL DEFAULT 0,
                created_at REAL DEFAULT (strftime('%s', 'now'))
            )
        ''')

        # Function pattern discoveries
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS function_patterns (
                pattern_id TEXT PRIMARY KEY,
                language TEXT NOT NULL,
                pattern_regex TEXT NOT NULL,
                function_count INTEGER DEFAULT 0,
                high_value_targets INTEGER DEFAULT 0,
                avg_complexity REAL DEFAULT 0.0,
                last_discovered REAL DEFAULT 0,
                created_at REAL DEFAULT (strftime('%s', 'now'))
            )
        ''')

        # Performance metrics
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_metrics (
                metric_id TEXT PRIMARY KEY,
                run_id TEXT NOT NULL,
                stage_name TEXT NOT NULL,
                execution_time REAL NOT NULL,
                cpu_usage REAL NOT NULL,
                memory_usage REAL NOT NULL,
                success_count INTEGER DEFAULT 0,
                error_count INTEGER DEFAULT 0,
                created_at REAL DEFAULT (strftime('%s', 'now')),
                FOREIGN KEY (run_id) REFERENCES run_history (run_id)
            )
        ''')

        # Learning insights
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_insights (
                insight_id TEXT PRIMARY KEY,
                insight_type TEXT NOT NULL,
                insight_data TEXT NOT NULL,
                confidence REAL NOT NULL,
                applied_count INTEGER DEFAULT 0,
                created_at REAL DEFAULT (strftime('%s', 'now'))
            )
        ''')

        # Create indexes for performance
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_language_patterns_lang ON language_patterns(language)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_keyword_effectiveness_lang ON keyword_effectiveness(language)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_function_patterns_lang ON function_patterns(language)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_performance_metrics_run ON performance_metrics(run_id)')

        conn.commit()

    def record_run_start(self, source_dir: str, config: Dict[str, Any]) -> str:
        """Record the start of a fuzzing run"""
        run_id = hashlib.md5(f"{source_dir}_{datetime.now(timezone.utc).timestamp()}".encode()).hexdigest()
        timestamp = datetime.now(timezone.utc).timestamp()

        conn = self.connection
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO run_history (run_id, timestamp, source_directory, total_files, processing_time, success_rate, config_json)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (run_id, timestamp, source_dir, 0, 0.0, 0.0, json.dumps(config)))

        conn.commit()
        return run_id

    def update_run_stats(self, run_id: str, total_files: int, processing_time: float, success_rate: float):
        """Update run statistics"""
        conn = self.connection
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE run_history
            SET total_files = ?, processing_time = ?, success_rate = ?
            WHERE run_id = ?
        ''', (total_files, processing_time, success_rate, run_id))
        conn.commit()

    def record_language_pattern(self, language: str, pattern_type: str, pattern_regex: str,
                               success: bool, confidence: float):
        """Record language detection pattern effectiveness"""
        pattern_id = hashlib.md5(f"{language}_{pattern_type}_{pattern_regex}".encode()).hexdigest()
        timestamp = datetime.now(timezone.utc).timestamp()

        conn = self.connection
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO language_patterns
            (pattern_id, language, pattern_type, pattern_regex, success_count, failure_count,
             avg_confidence, last_used)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (pattern_id, language, pattern_type, pattern_regex,
              1 if success else 0, 0 if success else 1, confidence, timestamp))

        conn.commit()

    def record_keyword_effectiveness(self, keyword: str, language: str, source_type: str,
                                   coverage_improvement: float):
        """Record dictionary keyword effectiveness"""
        keyword_id = hashlib.md5(f"{keyword}_{language}_{source_type}".encode()).hexdigest()
        timestamp = datetime.now(timezone.utc).timestamp()

        conn = self.connection
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO keyword_effectiveness
            (keyword_id, keyword, language, source_type, success_count,
             coverage_improvement, last_success)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (keyword_id, keyword, language, source_type, 1,
              coverage_improvement, timestamp))

        conn.commit()

    def get_top_keywords(self, language: str, limit: int = 20) -> List[Dict[str, Any]]:
        """Get most effective keywords for a language"""
        conn = self.connection
        cursor = conn.cursor()

        cursor.execute('''
            SELECT keyword, success_count, coverage_improvement, last_success
            FROM keyword_effectiveness
            WHERE language = ?
            ORDER BY coverage_improvement DESC, success_count DESC
            LIMIT ?
        ''', (language, limit))

        return [
            {
                'keyword': row[0],
                'success_count': row[1],
                'coverage_improvement': row[2],
                'last_success': row[3]
            }
            for row in cursor.fetchall()
        ]

    def get_successful_patterns(self, language: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Get most successful language patterns"""
        conn = self.connection
        cursor = conn.cursor()

        cursor.execute('''
            SELECT pattern_regex, success_count, avg_confidence, last_used
            FROM language_patterns
            WHERE language = ? AND success_count > failure_count
            ORDER BY success_count DESC, avg_confidence DESC
            LIMIT ?
        ''', (language, limit))

        return [
            {
                'pattern': row[0],
                'success_count': row[1],
                'confidence': row[2],
                'last_used': row[3]
            }
            for row in cursor.fetchall()
        ]

    def record_insight(self, insight_type: str, insight_data: Dict[str, Any], confidence: float):
        """Record a learning insight"""
        insight_id = hashlib.md5(f"{insight_type}_{json.dumps(insight_data, sort_keys=True)}".encode()).hexdigest()

        conn = self.connection
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO learning_insights
            (insight_id, insight_type, insight_data, confidence)
            VALUES (?, ?, ?, ?)
        ''', (insight_id, insight_type, json.dumps(insight_data), confidence))

        conn.commit()

    def get_insights(self, insight_type: str = None, limit: int = 50) -> List[Dict[str, Any]]:
        """Get learning insights"""
        conn = self.connection
        cursor = conn.cursor()

        if insight_type:
            cursor.execute('''
                SELECT insight_type, insight_data, confidence, applied_count, created_at
                FROM learning_insights
                WHERE insight_type = ?
                ORDER BY confidence DESC, applied_count DESC
                LIMIT ?
            ''', (insight_type, limit))
        else:
            cursor.execute('''
                SELECT insight_type, insight_data, confidence, applied_count, created_at
                FROM learning_insights
                ORDER BY confidence DESC, applied_count DESC
                LIMIT ?
            ''', (limit,))

        return [
            {
                'type': row[0],
                'data': json.loads(row[1]),
                'confidence': row[2],
                'applied_count': row[3],
                'created_at': row[4]
            }
            for row in cursor.fetchall()
        ]

    def close_all_connections(self):
        """Close all database connections"""
        if hasattr(self._local, 'conn') and self._local.conn:
            self._local.conn.close()
            self._local.conn = None

# Phase 7: High-Performance State Management
try:
    import redis  # type: ignore
    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False

import json
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Union
import hashlib
import atexit
import threading

class FuzzerStateManager:
    """High-performance state management with Redis LRU/LFU caching"""

    def __init__(self, redis_host: str = 'localhost', redis_port: int = 6379,
                 cache_size: int = 10, state_file: str = 'fuzzer_state.json'):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.cache_size = cache_size
        self.state_file = Path(state_file)
        self._local = threading.local()
        self._in_memory_cache: Dict[str, List[Dict]] = {}

        # Initialize Redis connection
        self._init_redis()

        # Load persisted state if available
        self._load_persisted_state()

        # Register shutdown handler
        atexit.register(self._save_state_on_shutdown)

    @property
    def redis_client(self):
        """Thread-local Redis client with fallback"""
        if not HAS_REDIS:
            return None

        if not hasattr(self._local, 'redis') or self._local.redis is None:
            try:
                self._local.redis = redis.Redis(
                    host=self.redis_host,
                    port=self.redis_port,
                    decode_responses=True
                )
                # Test connection
                self._local.redis.ping()
            except Exception:
                print(f"âš ï¸  Redis not available at {self.redis_host}:{self.redis_port}")
                print("ðŸ”„ Falling back to in-memory cache only")
                self._local.redis = None

        return self._local.redis

    def _init_redis(self):
        """Initialize Redis connection with fallback"""
        try:
            client = redis.Redis(
                host=self.redis_host,
                port=self.redis_port,
                decode_responses=True
            )
            client.ping()
            print(f"âœ… Connected to Redis at {self.redis_host}:{self.redis_port}")
        except redis.ConnectionError:
            print(f"âš ï¸  Redis not available, will use in-memory fallback")

    def add_task_state(self, task_id: str, state_data: Dict[str, Any]) -> bool:
        """Add state data for a fuzzing task with intelligent caching"""
        try:
            # Generate hash for the data
            data_hash = hashlib.sha256(
                json.dumps(state_data, sort_keys=True).encode()
            ).hexdigest()

            # Check if we already have this exact data
            existing_key = self._find_existing_state(task_id, data_hash)
            current_time = datetime.now(timezone.utc).timestamp()

            if existing_key:
                # Update existing entry (cache hit)
                return self._update_existing_state(task_id, existing_key, current_time)
            else:
                # Add new entry (cache miss)
                return self._add_new_state(task_id, state_data, data_hash, current_time)

        except Exception as e:
            print(f"âš ï¸  Failed to add task state: {e}")
            return False

    def get_task_states(self, task_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Get cached states for a task, ordered by relevance (LRU/LFU)"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return []

            key_pattern = f"fuzzed_task:{task_id}"
            states = []

            # Get all members of the sorted set
            members = redis_client.zrange(key_pattern, 0, limit - 1, desc=True, withscores=True)

            for member_json, score in members:
                try:
                    state_data = json.loads(member_json)
                    state_data['_cache_metadata'] = {
                        'last_accessed_ts': score,
                        'relevance_score': score
                    }
                    states.append(state_data)
                except json.JSONDecodeError:
                    continue

            return states

        except Exception as e:
            print(f"âš ï¸  Failed to get task states: {e}")
            return []

    def _find_existing_state(self, task_id: str, data_hash: str) -> Optional[str]:
        """Find existing state by data hash"""
        redis_client = self.redis_client
        if not redis_client:
            return None

        key_pattern = f"fuzzed_task:{task_id}"

        # Get all members and check their hashes
        members = redis_client.zrange(key_pattern, 0, -1)
        for member in members:
            try:
                state_data = json.loads(member)
                if state_data.get('data_hash') == data_hash:
                    return member
            except json.JSONDecodeError:
                continue

        return None

    def _update_existing_state(self, task_id: str, existing_member: str, current_time: float) -> bool:
        """Update existing state with new access time and increment counter"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return False

            key_pattern = f"fuzzed_task:{task_id}"

            # Parse existing data
            existing_data = json.loads(existing_member)

            # Increment access count
            existing_data['access_count'] = existing_data.get('access_count', 0) + 1
            existing_data['last_accessed_ts'] = current_time

            # Update in Redis
            redis_client.zrem(key_pattern, existing_member)
            new_member = json.dumps(existing_data)
            redis_client.zadd(key_pattern, {new_member: current_time})

            return True

        except Exception as e:
            print(f"âš ï¸  Failed to update existing state: {e}")
            return False

    def _add_new_state(self, task_id: str, state_data: Dict[str, Any],
                      data_hash: str, current_time: float) -> bool:
        """Add new state to cache with eviction logic"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return False

            key_pattern = f"fuzzed_task:{task_id}"

            # Create advanced LRU wrapper
            wrapper_data = {
                'last_accessed_ts': current_time,
                'created_ts': current_time,
                'access_count': 1,
                'data_hash': data_hash,
                'data': state_data
            }

            # Add to Redis
            member_json = json.dumps(wrapper_data)
            redis_client.zadd(key_pattern, {member_json: current_time})

            # Enforce cache size limit (LRU eviction)
            self._enforce_cache_limit(key_pattern)

            return True

        except Exception as e:
            print(f"âš ï¸  Failed to add new state: {e}")
            return False

    def _enforce_cache_limit(self, key_pattern: str):
        """Enforce cache size limit using LRU eviction"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return

            # Get current size
            size = redis_client.zcard(key_pattern)

            if size > self.cache_size:
                # Remove oldest entries (lowest scores)
                excess = size - self.cache_size
                redis_client.zremrangebyrank(key_pattern, 0, excess - 1)

        except Exception as e:
            print(f"âš ï¸  Failed to enforce cache limit: {e}")

    def _load_persisted_state(self):
        """Load persisted state from JSON file"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    state_data = json.load(f)

                redis_client = self.redis_client
                if redis_client:
                    # Load each task's states
                    for task_id, states in state_data.items():
                        key_pattern = f"fuzzed_task:{task_id}"
                        for state in states:
                            member_json = json.dumps(state)
                            score = state.get('last_accessed_ts', datetime.now(timezone.utc).timestamp())
                            redis_client.zadd(key_pattern, {member_json: score})

                    print(f"âœ… Loaded persisted state for {len(state_data)} tasks")
                else:
                    print(f"â„¹ï¸  State file found but Redis unavailable - state will be loaded on Redis connection")
            else:
                print(f"â„¹ï¸  No persisted state file found at {self.state_file}")

        except Exception as e:
            print(f"âš ï¸  Failed to load persisted state: {e}")

    def _save_state_on_shutdown(self):
        """Save complete cache state to JSON file on shutdown"""
        try:
            print("ðŸ’¾ Saving fuzzer state to disk...")

            redis_client = self.redis_client
            if not redis_client:
                print("âš ï¸  No Redis connection - skipping state save")
                return

            # Get all task keys
            keys = redis_client.keys("fuzzed_task:*")
            state_data = {}

            for key in keys:
                task_id = key.replace("fuzzed_task:", "")
                members = redis_client.zrange(key, 0, -1, desc=True)

                task_states = []
                for member in members:
                    try:
                        state = json.loads(member)
                        task_states.append(state)
                    except json.JSONDecodeError:
                        continue

                if task_states:
                    state_data[task_id] = task_states

            # Save to file
            with open(self.state_file, 'w') as f:
                json.dump(state_data, f, indent=2, default=str)

            print(f"âœ… Saved state for {len(state_data)} tasks to {self.state_file}")

        except Exception as e:
            print(f"âš ï¸  Failed to save state on shutdown: {e}")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return {'status': 'redis_unavailable'}

            keys = redis_client.keys("fuzzed_task:*")
            total_entries = 0
            task_stats = {}

            for key in keys:
                task_id = key.replace("fuzzed_task:", "")
                size = redis_client.zcard(key)
                total_entries += size

                # Get most recent and oldest entries
                newest = redis_client.zrange(key, -1, -1, withscores=True)
                oldest = redis_client.zrange(key, 0, 0, withscores=True)

                task_stats[task_id] = {
                    'cached_entries': size,
                    'newest_entry_ts': newest[0][1] if newest else None,
                    'oldest_entry_ts': oldest[0][1] if oldest else None
                }

            return {
                'status': 'healthy',
                'total_tasks': len(keys),
                'total_entries': total_entries,
                'task_breakdown': task_stats,
                'cache_size_limit': self.cache_size
            }

        except Exception as e:
            return {'status': 'error', 'error': str(e)}

    def clear_cache(self, task_id: Optional[str] = None):
        """Clear cache for specific task or all tasks"""
        try:
            redis_client = self.redis_client
            if not redis_client:
                return False

            if task_id:
                key_pattern = f"fuzzed_task:{task_id}"
                redis_client.delete(key_pattern)
                print(f"âœ… Cleared cache for task {task_id}")
            else:
                keys = redis_client.keys("fuzzed_task:*")
                if keys:
                    redis_client.delete(*keys)
                print(f"âœ… Cleared cache for all {len(keys)} tasks")

            return True

        except Exception as e:
            print(f"âš ï¸  Failed to clear cache: {e}")
            return False

# Phase 4: Advanced Reporting & Professional Output
try:
    import matplotlib  # type: ignore
    matplotlib.use('Agg')  # Use non-interactive backend
    import matplotlib.pyplot as plt  # type: ignore
    import matplotlib.dates as mdates  # type: ignore
    from matplotlib.backends.backend_pdf import PdfPages  # type: ignore
    import seaborn as sns  # type: ignore
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    from jinja2 import Template
    HAS_JINJA2 = True
except ImportError:
    HAS_JINJA2 = False

# =============================================================================
# PHASE 5: EXTENSIBLE PLUGIN-BASED ARCHITECTURE
# =============================================================================

# Plugin Interfaces
from abc import ABC, abstractmethod

@dataclass
class PluginResult:
    """Standardized result format for all plugins"""
    success: bool
    data: Any
    confidence: float = 1.0
    metadata: Optional[Dict[str, Any]] = None
    errors: Optional[List[str]] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.errors is None:
            self.errors = []

class LanguagePlugin(ABC):
    """Abstract base class for language detection and analysis plugins"""

    @property
    @abstractmethod
    def name(self) -> str:
        """Plugin name"""
        pass

    @property
    @abstractmethod
    def supported_extensions(self) -> List[str]:
        """File extensions this plugin can handle"""
        pass

    @property
    @abstractmethod
    def language_name(self) -> str:
        """Human-readable language name"""
        pass

    @abstractmethod
    def detect_language(self, file_path: Path) -> PluginResult:
        """Detect if a file is written in this language"""
        pass

    @abstractmethod
    def analyze_content(self, file_path: Path) -> PluginResult:
        """Analyze file content for language-specific patterns"""
        pass

    @abstractmethod
    def extract_keywords(self, file_path: Path) -> PluginResult:
        """Extract language-specific keywords and patterns"""
        pass

class ReportingPlugin(ABC):
    """Abstract base class for report generation plugins"""

    @property
    @abstractmethod
    def name(self) -> str:
        """Plugin name"""
        pass

    @property
    @abstractmethod
    def output_format(self) -> str:
        """Output format (html, pdf, json, etc.)"""
        pass

    @abstractmethod
    def generate_report(self, results: Dict, analytics: Dict, recommendations: List[str],
                       output_path: Path) -> PluginResult:
        """Generate a report in the specified format"""
        pass

class AnalysisPlugin(ABC):
    """Abstract base class for analysis plugins"""

    @property
    @abstractmethod
    def name(self) -> str:
        """Plugin name"""
        pass

    @property
    @abstractmethod
    def analysis_type(self) -> str:
        """Type of analysis this plugin performs"""
        pass

    @abstractmethod
    def analyze(self, data: Any) -> PluginResult:
        """Perform analysis on the provided data"""
        pass

class SecurityAnalysisPlugin(AnalysisPlugin):
    """Specialized plugin for security-focused analysis"""

    @abstractmethod
    def scan_vulnerabilities(self, source_files: List[Path]) -> PluginResult:
        """Scan for potential security vulnerabilities"""
        pass

    @abstractmethod
    def generate_security_report(self, findings: Dict) -> PluginResult:
        """Generate security-focused report"""
        pass

class PluginManager:
    """Dynamic plugin discovery and management system with enhanced security"""

    def __init__(self, plugins_dir: Path = None):
        if plugins_dir is None:
            # Default to plugins directory relative to this file
            current_dir = Path(__file__).parent
            plugins_dir = current_dir / "plugins"

        self.plugins_dir = plugins_dir
        self.plugins_dir.mkdir(exist_ok=True)

        self.language_plugins: Dict[str, LanguagePlugin] = {}
        self.reporting_plugins: Dict[str, ReportingPlugin] = {}
        self.analysis_plugins: Dict[str, AnalysisPlugin] = {}
        self.security_plugins: Dict[str, SecurityAnalysisPlugin] = {}

        # Plugin validation and security
        self.trusted_plugins: set = set()
        self.plugin_signatures: Dict[str, str] = {}

        self.discover_plugins()

    def discover_plugins(self):
        """Discover and load all plugins from the plugins directory with security validation"""
        if not self.plugins_dir.exists():
            print(f"âš ï¸  Plugins directory not found: {self.plugins_dir}")
            return

        print(f"ðŸ” Discovering plugins in: {self.plugins_dir}")

        for plugin_file in self.plugins_dir.glob("*.py"):
            if plugin_file.name.startswith("_"):
                continue  # Skip private files

            try:
                # Validate plugin before loading
                if self._validate_plugin_security(plugin_file):
                    self._load_plugin(plugin_file)
                else:
                    print(f"âš ï¸  Plugin security validation failed: {plugin_file.name}")
            except Exception as e:
                print(f"âŒ Failed to load plugin {plugin_file.name}: {e}")

    def _validate_plugin_security(self, plugin_file: Path) -> bool:
        """Validate plugin security before loading"""
        try:
            # Basic security checks
            content = plugin_file.read_text()

            # Check for potentially dangerous imports/operations
            dangerous_patterns = [
                r'import\s+os\s*\.\s*system',
                r'subprocess\s*\.\s*call',
                r'eval\s*\(',
                r'exec\s*\(',
                r'__import__\s*\(',
                r'open\s*\([^)]*["\']w["\']',  # Write operations
            ]

            for pattern in dangerous_patterns:
                if re.search(pattern, content):
                    print(f"âš ï¸  Potentially unsafe pattern detected in {plugin_file.name}: {pattern}")
                    return False

            # Calculate and store plugin signature
            plugin_hash = hashlib.sha256(content.encode()).hexdigest()
            self.plugin_signatures[plugin_file.name] = plugin_hash

            return True
        except Exception as e:
            print(f"âŒ Plugin validation error for {plugin_file.name}: {e}")
            return False

    def _load_plugin(self, plugin_file: Path):
        """Load a single plugin file with enhanced error handling"""
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            f"plugins.{plugin_file.stem}",
            plugin_file
        )

        if spec is None or spec.loader is None:
            raise ImportError(f"Could not load plugin spec: {plugin_file}")

        module = importlib.util.module_from_spec(spec)

        try:
            spec.loader.exec_module(module)
        except Exception as e:
            raise ImportError(f"Failed to execute plugin module: {e}")

        # Look for plugin classes in the module
        plugins_loaded = 0
        for attr_name in dir(module):
            attr = getattr(module, attr_name)
            if isinstance(attr, type):
                # Check if it inherits from our plugin base classes
                if issubclass(attr, LanguagePlugin) and attr != LanguagePlugin:
                    try:
                        plugin = attr()
                        self.language_plugins[plugin.name] = plugin
                        print(f"âœ… Loaded language plugin: {plugin.name} ({plugin.language_name})")
                        plugins_loaded += 1
                    except Exception as e:
                        print(f"âŒ Failed to instantiate language plugin {attr}: {e}")

                elif issubclass(attr, SecurityAnalysisPlugin) and attr != SecurityAnalysisPlugin:
                    try:
                        plugin = attr()
                        self.security_plugins[plugin.name] = plugin
                        print(f"âœ… Loaded security plugin: {plugin.name} ({plugin.analysis_type})")
                        plugins_loaded += 1
                    except Exception as e:
                        print(f"âŒ Failed to instantiate security plugin {attr}: {e}")

                elif issubclass(attr, ReportingPlugin) and attr != ReportingPlugin:
                    try:
                        plugin = attr()
                        self.reporting_plugins[plugin.name] = plugin
                        print(f"âœ… Loaded reporting plugin: {plugin.name} ({plugin.output_format})")
                        plugins_loaded += 1
                    except Exception as e:
                        print(f"âŒ Failed to instantiate reporting plugin {attr}: {e}")

                elif issubclass(attr, AnalysisPlugin) and attr != AnalysisPlugin:
                    try:
                        plugin = attr()
                        self.analysis_plugins[plugin.name] = plugin
                        print(f"âœ… Loaded analysis plugin: {plugin.name} ({plugin.analysis_type})")
                        plugins_loaded += 1
                    except Exception as e:
                        print(f"âŒ Failed to instantiate analysis plugin {attr}: {e}")

        if plugins_loaded > 0:
            self.trusted_plugins.add(plugin_file.name)

    def get_language_plugin_for_file(self, file_path: Path) -> Optional[LanguagePlugin]:
        """Get the appropriate language plugin for a file based on extension"""
        extension = file_path.suffix.lower()

        for plugin in self.language_plugins.values():
            if extension in plugin.supported_extensions:
                return plugin

        return None

    def get_reporting_plugin(self, format_name: str) -> Optional[ReportingPlugin]:
        """Get a reporting plugin by format name"""
        return self.reporting_plugins.get(format_name)

    def get_security_plugins(self) -> List[SecurityAnalysisPlugin]:
        """Get all available security analysis plugins"""
        return list(self.security_plugins.values())

    def list_plugins(self) -> Dict[str, List[str]]:
        """List all loaded plugins by type"""
        return {
            'language': list(self.language_plugins.keys()),
            'reporting': list(self.reporting_plugins.keys()),
            'analysis': list(self.analysis_plugins.keys()),
            'security': list(self.security_plugins.keys())
        }

    def get_plugin_info(self) -> Dict[str, Any]:
        """Get detailed plugin information"""
        return {
            'total_plugins': len(self.language_plugins) + len(self.reporting_plugins) +
                           len(self.analysis_plugins) + len(self.security_plugins),
            'trusted_plugins': len(self.trusted_plugins),
            'plugin_signatures': self.plugin_signatures,
            'plugins_by_type': self.list_plugins()
        }

# Phase 3: Intelligent Automation Classes (Mock implementations for compatibility)
class IntelligentDetector:
    """Mock intelligent detector for compatibility"""
    def __init__(self):
        pass
    
    def detect_language_advanced(self, file_path: Path) -> Dict[str, Any]:
        """Mock advanced language detection"""
        return {
            'language': 'unknown',
            'confidence': 0.5,
            'patterns': []
        }
    
    def analyze_fuzz_target_potential(self, file_path: Path) -> Dict[str, Any]:
        """Mock fuzz target analysis"""
        return {
            'probability': 0.5,
            'reasons': []
        }

class AutoOptimizer:
    """Mock auto optimizer for compatibility"""
    def __init__(self, analytics_logger):
        self.analytics_logger = analytics_logger
        self.optimization_cycles = 0
    
    def optimize_parameters(self, current_metrics: Dict) -> Any:
        """Mock parameter optimization"""
        self.optimization_cycles += 1
        # Return a mock optimization result
        class OptimizedParams:
            def __init__(self):
                self.max_workers = current_metrics.get('max_workers', 4)
        return OptimizedParams()
    
    def generate_intelligent_recommendations(self, stats) -> List[str]:
        """Mock intelligent recommendations"""
        return ["Consider enabling more parallel workers for better performance"]

class SmartDictionaryGenerator:
    """Mock smart dictionary generator for compatibility"""
    def __init__(self):
        pass
    
    def generate_smart_dictionary(self, source_files: List[Path], languages: List[str]) -> List[str]:
        """Mock smart dictionary generation"""
        return ["test", "input", "data", "fuzz", "target"]

class IntelligenceMetrics:
    """Mock intelligence metrics for compatibility"""
    def __init__(self):
        self.language_confidence = 0.0
        self.fuzz_target_probability = 0.0
        self.dictionary_relevance = 0.0
        self.optimization_score = 0.0

class ProfessionalReportGenerator:
    """Mock professional report generator for compatibility"""
    def __init__(self, project_name: str, project_dir: Path):
        self.project_name = project_name
        self.project_dir = project_dir
    
    def generate_comprehensive_report(self, results: Dict, analytics: Dict, recommendations: List[str]) -> Dict[str, str]:
        """Mock comprehensive report generation"""
        return {}

class CoreEngine:
    """The core engine that coordinates all fuzzing expansion activities with enhanced security"""

    def __init__(self, plugin_manager: PluginManager, config: Dict):
        self.plugin_manager = plugin_manager
        self.config = config

        # Initialize the sentient core - persistent analytics database
        self.analytics_db = AnalyticsDatabase()

        # Create logs directory and ensure it exists
        log_dir = Path(config.get('project_dir', '.')) / "logs"
        log_dir.mkdir(exist_ok=True, parents=True)
        self.analytics_logger = AnalyticsLogger(log_dir / "core_engine.log")
        self.parallel_processor = ParallelProcessor(
            max_workers=config.get('max_workers', 4),
            analytics_logger=self.analytics_logger
        )

        # Security enhancements
        self.security_findings: List[Dict] = []
        self.risk_assessment: Dict[str, Any] = {}

        # Learning and adaptation
        self.run_id = None
        self.learning_insights = {}
        self._load_learning_insights()

        # High-performance state management
        self.state_manager = FuzzerStateManager()

        # Multi-source intelligence gathering
        self.intelligence_sources = {
            'google': 'https://bughunters.google.com/feed/en',
            'cisa': 'https://www.cisa.gov/known-exploited-vulnerabilities-catalog/rss.xml',
            'msrc': 'https://msrc.microsoft.com/update-guide/rss',
            'ibm': 'https://exchange.xforce.ibmcloud.com/rss'
        }
        self.intelligence_briefing = {}

    def _load_learning_insights(self):
        """Load learning insights from the database for intelligent decision making"""
        print("ðŸ§  Loading learning insights from database...")

        try:
            insights = self.analytics_db.get_insights(limit=100)

            # Organize insights by type for easy access
            for insight in insights:
                insight_type = insight['type']
                if insight_type not in self.learning_insights:
                    self.learning_insights[insight_type] = []

                self.learning_insights[insight_type].append({
                    'data': insight['data'],
                    'confidence': insight['confidence'],
                    'applied_count': insight['applied_count']
                })

            print(f"âœ… Loaded {len(insights)} learning insights across {len(self.learning_insights)} categories")

            # Log top insights for debugging
            for insight_type, insights_list in self.learning_insights.items():
                if insights_list:
                    top_insight = max(insights_list, key=lambda x: x['confidence'])
                    print(f"ðŸ§  Top {insight_type} insight (confidence: {top_insight['confidence']:.2f})")

        except Exception as e:
            print(f"âš ï¸  Could not load learning insights: {e}")
            self.learning_insights = {}

    def _apply_learning_insights(self):
        """Apply learning insights to make smarter decisions for this run"""
        print("ðŸ§  Applying learning insights for optimal performance...")

        # Adjust worker allocation based on language performance history
        language_insights = self.learning_insights.get('language_performance', [])
        if language_insights:
            # Sort languages by historical success rate
            sorted_languages = sorted(
                language_insights,
                key=lambda x: x['data'].get('success_rate', 0),
                reverse=True
            )

            # Adjust max_workers based on top-performing language
            if sorted_languages:
                top_language = sorted_languages[0]['data'].get('language', 'unknown')
                success_rate = sorted_languages[0]['data'].get('success_rate', 0)
                print(f"ðŸ§  Prioritizing {top_language} (historical success rate: {success_rate:.1%})")

                # If a language has high historical success, allocate more resources
                if success_rate > 0.7:
                    original_workers = self.parallel_processor.max_workers
                    boosted_workers = min(original_workers * 2, 16)  # Cap at 16
                    if boosted_workers > original_workers:
                        print(f"ðŸ§  Boosting worker count from {original_workers} to {boosted_workers}")
                        self.parallel_processor.max_workers = boosted_workers

        # Apply keyword insights for better dictionary generation
        keyword_insights = self.learning_insights.get('keyword_effectiveness', [])
        if keyword_insights:
            print(f"ðŸ§  Found {len(keyword_insights)} keyword effectiveness insights")

            # The keyword insights will be used later in the content analysis phase
            # to prioritize the most effective keywords for fuzzing

        print("âœ… Learning insights applied successfully")

    def _run_intelligence_gathering(self):
        """Gather intelligence from all configured security sources"""
        print("ðŸ“° Gathering intelligence from all configured sources...")
        self.intelligence_briefing['items'] = []

        trend_plugin = self.plugin_manager.analysis_plugins.get("SecurityTrendAnalyzer")
        if not trend_plugin:
            print("âš ï¸  SecurityTrendAnalyzer plugin not found. Skipping intelligence gathering.")
            return

        for source_name, url in self.intelligence_sources.items():
            try:
                print(f"   -> Fetching from {source_name.upper()}...")

                # In a real implementation, this would use requests to fetch the actual RSS
                # For now, we'll use mock data to demonstrate the functionality
                mock_rss_content = self._get_mock_rss_content(source_name)

                if mock_rss_content:
                    result = trend_plugin.analyze({
                        'rss_content': mock_rss_content,
                        'source_name': source_name
                    })
                    if result.success:
                        items = result.data.get('items', [])
                        self.intelligence_briefing['items'].extend(items)
                        print(f"   âœ… Processed {len(items)} items from {source_name.upper()}.")
                    else:
                        print(f"   âŒ Failed to analyze {source_name.upper()}: {result.error}")
                else:
                    print(f"   âš ï¸  No RSS content available for {source_name.upper()}")

            except Exception as e:
                print(f"   âŒ Failed to process feed from {source_name.upper()}: {e}")

        # Create comprehensive briefing
        self.intelligence_briefing.update({
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'total_sources': len(self.intelligence_sources),
            'total_items': len(self.intelligence_briefing['items']),
            'briefing_type': 'multi_source_security_intelligence'
        })

        # Log the aggregated results
        self.analytics_logger.log_event(
            'INTELLIGENCE_GATHERING_COMPLETE',
            f'Aggregated {len(self.intelligence_briefing["items"])} items from {len(self.intelligence_sources)} sources.',
            briefing=self.intelligence_briefing
        )

        print(f"ðŸ§  Intelligence gathering complete. {len(self.intelligence_briefing['items'])} items from {len(self.intelligence_sources)} sources.")

    def _get_mock_rss_content(self, source_name: str) -> str:
        """Get mock RSS content for demonstration purposes"""
        # In a real implementation, this would fetch from the actual URLs
        if source_name == 'google':
            return '''<?xml version="1.0"?>
            <rss version="2.0">
                <channel>
                    <item>
                        <title>Security: Protecting Large Language Models from Prompt Injection</title>
                        <description>Recent research shows that LLMs are vulnerable to carefully crafted prompts that can cause them to generate harmful content or reveal sensitive information.</description>
                                                  <link>https://bughunters.google.com/security</link>
                        <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                    </item>
                    <item>
                        <title>Cloud Security: Kubernetes Vulnerabilities on the Rise</title>
                        <description>Attackers are increasingly targeting misconfigured Kubernetes clusters, leading to container escapes and data breaches.</description>
                        <link>https://bughunters.google.com/cloud-security</link>
                        <pubDate>Mon, 01 Jan 2024 10:00:00 GMT</pubDate>
                    </item>
                </channel>
            </rss>'''
        elif source_name == 'cisa':
            return '''<?xml version="1.0"?>
            <rss version="2.0">
                <channel>
                    <item>
                        <title>CVE-2023-12345: Microsoft Exchange Remote Code Execution Vulnerability</title>
                        <description>This vulnerability allows remote attackers to execute arbitrary code on affected systems. Currently being actively exploited.</description>
                        <link>https://www.cisa.gov/cve-2023-12345</link>
                        <pubDate>Mon, 01 Jan 2024 08:00:00 GMT</pubDate>
                    </item>
                    <item>
                        <title>CVE-2023-67890: Apache Log4j JNDI Injection</title>
                        <description>A critical vulnerability in Apache Log4j that allows remote code execution. This is a high-priority threat.</description>
                        <link>https://www.cisa.gov/cve-2023-67890</link>
                        <pubDate>Sun, 31 Dec 2023 22:00:00 GMT</pubDate>
                    </item>
                </channel>
            </rss>'''
        elif source_name == 'msrc':
            return '''<?xml version="1.0"?>
            <rss version="2.0">
                <channel>
                    <item>
                        <title>CVE-2024-12345 | Microsoft Windows Remote Code Execution Vulnerability</title>
                        <description>A remote code execution vulnerability exists in Microsoft Windows when it fails to properly handle certain types of input.</description>
                        <link>https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-12345</link>
                        <pubDate>Mon, 01 Jan 2024 06:00:00 GMT</pubDate>
                    </item>
                    <item>
                        <title>CVE-2024-67890 | Microsoft Office Security Feature Bypass</title>
                        <description>An attacker could bypass security features in Microsoft Office by exploiting this vulnerability.</description>
                        <link>https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-67890</link>
                        <pubDate>Mon, 01 Jan 2024 04:00:00 GMT</pubDate>
                    </item>
                </channel>
            </rss>'''
        elif source_name == 'ibm':
            return '''<?xml version="1.0"?>
            <rss version="2.0">
                <channel>
                    <item>
                        <title>New Rust-based Malware Campaign Targets Cloud Infrastructure</title>
                        <description>IBM X-Force has identified a sophisticated campaign using Rust-based malware to compromise cloud infrastructure, particularly targeting Kubernetes clusters.</description>
                        <link>https://exchange.xforce.ibmcloud.com/malware/rust-campaign</link>
                        <pubDate>Mon, 01 Jan 2024 02:00:00 GMT</pubDate>
                    </item>
                    <item>
                        <title>Supply Chain Attack Uses Compromised Open-Source Libraries</title>
                        <description>Attackers are compromising popular open-source libraries to distribute malware through software supply chains.</description>
                        <link>https://exchange.xforce.ibmcloud.com/threat/supply-chain-attack</link>
                        <pubDate>Sun, 31 Dec 2023 18:00:00 GMT</pubDate>
                    </item>
                </channel>
            </rss>'''
        return None

    def _track_fuzzing_task(self, task_name: str, task_data: Dict[str, Any]):
        """Track a fuzzing task state using the high-performance state manager"""
        try:
            # Add task state with intelligent caching
            success = self.state_manager.add_task_state(task_name, task_data)

            if success:
                # Get updated cache stats
                cache_stats = self.state_manager.get_cache_stats()
                if cache_stats.get('status') == 'healthy':
                    print(f"ðŸ§  Cached task state for '{task_name}' (cache size: {cache_stats.get('total_entries', 0)})")

        except Exception as e:
            print(f"âš ï¸  Failed to track task state: {e}")

    def _generate_learning_insights(self, artifacts: Dict, language_results: Dict, analytics_report: Dict):
        """Generate learning insights from the current run for future optimization"""
        print("ðŸ§  Generating learning insights from this run...")

        try:
            # Generate language performance insights
            languages = language_results.get('languages', {})
            for lang, lang_data in languages.items():
                success_rate = lang_data.get('file_count', 0) / max(len(language_results.get('files_analyzed', [])), 1)

                insight_data = {
                    'language': lang,
                    'success_rate': success_rate,
                    'file_count': lang_data.get('file_count', 0),
                    'patterns_found': len(lang_data.get('patterns', [])),
                    'processing_time': analytics_report.get('total_runtime_seconds', 0)
                }

                confidence = min(success_rate * 0.8 + 0.2, 1.0)  # Base confidence on success rate
                self.analytics_db.record_insight('language_performance', insight_data, confidence)

            # Generate keyword effectiveness insights
            keyword_insights = self._analyze_keyword_effectiveness(artifacts)
            for keyword_data in keyword_insights:
                self.analytics_db.record_insight('keyword_effectiveness', keyword_data, 0.7)

            # Generate resource optimization insights
            resource_data = {
                'worker_count': self.parallel_processor.max_workers,
                'processing_time': analytics_report.get('total_runtime_seconds', 0),
                'files_processed': len(language_results.get('files_analyzed', [])),
                'efficiency': len(artifacts.get('fuzzers', [])) / max(analytics_report.get('total_runtime_seconds', 1), 1)
            }
            self.analytics_db.record_insight('resource_optimization', resource_data, 0.6)

            print(f"âœ… Generated learning insights for {len(languages)} languages and resource optimization")

        except Exception as e:
            print(f"âš ï¸  Failed to generate learning insights: {e}")

    def _analyze_keyword_effectiveness(self, artifacts: Dict) -> List[Dict]:
        """Analyze which keywords were most effective in this run"""
        keyword_analysis = []

        # Extract keywords from generated dictionaries
        dictionaries = artifacts.get('dictionaries', [])
        for dict_path in dictionaries:
            if dict_path.exists():
                try:
                    with open(dict_path, 'r', encoding='utf-8', errors='ignore') as f:
                        keywords = [line.strip() for line in f if line.strip() and not line.startswith('#')]

                    # Simulate coverage improvement analysis (in real implementation, this would
                    # be based on actual fuzzing results and code coverage data)
                    for keyword in keywords[:50]:  # Limit analysis to top keywords
                        coverage_improvement = hash(keyword) % 100 / 100.0  # Mock value

                        keyword_analysis.append({
                            'keyword': keyword,
                            'source': 'generated_dictionary',
                            'coverage_improvement': coverage_improvement,
                            'usage_count': 1,
                            'language': self._guess_keyword_language(keyword)
                        })
                except Exception as e:
                    print(f"âš ï¸  Could not analyze dictionary {dict_path}: {e}")

        return keyword_analysis

    def _guess_keyword_language(self, keyword: str) -> str:
        """Guess the programming language context of a keyword"""
        # Simple heuristic-based language detection for keywords
        go_keywords = ['func', 'package', 'import', 'defer', 'go', 'chan', 'goroutine']
        js_keywords = ['function', 'const', 'let', 'var', 'require', 'export', 'async', 'await', 'Promise']
        java_keywords = ['public', 'class', 'static', 'void', 'import', 'extends', 'implements']
        python_keywords = ['def', 'import', 'class', 'self', 'print', 'lambda', 'yield', 'with']

        if any(gk in keyword.lower() for gk in go_keywords):
            return 'go'
        elif any(jk in keyword.lower() for jk in js_keywords):
            return 'javascript'
        elif any(jk in keyword.lower() for jk in java_keywords):
            return 'java'
        elif any(pk in keyword.lower() for pk in python_keywords):
            return 'python'

        return 'unknown'

    def run_expansion(self, source_dir_or_fs, output_dir: Path) -> Dict:
        """Run the complete fuzzing expansion process with security analysis and learning"""
        print(f"ðŸš€ Core Engine starting expansion...")
        print(f"ðŸ“ Source: {source_dir_or_fs}")
        print(f"ðŸ“ Output: {output_dir}")

        # Initialize analytics_report early to avoid scope issues
        analytics_report = {'total_runtime_seconds': 0, 'processing_rate_mbps': 0, 'success_rate_percent': 100}

        # Handle both Path and ScopedFileSystem inputs
        if hasattr(source_dir_or_fs, 'glob') and hasattr(source_dir_or_fs, 'read_text'):
            # This is a ScopedFileSystem object
            self.scoped_fs = source_dir_or_fs
            self.source_dir = source_dir_or_fs.root
            print("ðŸ›¡ï¸  Operating in compliance-scoped mode")
        else:
            # This is a regular Path object
            self.scoped_fs = None
            self.source_dir = Path(source_dir_or_fs)
            print("ðŸ”“ Operating in unrestricted mode")

        # Record run start in analytics database
        self.run_id = self.analytics_db.record_run_start(str(self.source_dir), self.config)
        print(f"ðŸ§  Run ID: {self.run_id}")

        # Gather intelligence from multiple security sources
        self._run_intelligence_gathering()

        # Apply learning insights for smarter resource allocation
        self._apply_learning_insights()

        # Initialize analytics
        self.analytics_logger.start_performance_tracking()

        try:
            # Phase 1: Security Analysis (if enabled)
            security_results = {}
            if self.config.get('enable_security_analysis', False):
                security_results = self._run_security_analysis(self.source_dir)

            # Phase 2: Language Detection and Analysis
            language_results = self._run_language_analysis(self.source_dir)

            # Track language analysis results
            self._track_fuzzing_task('language_analysis', {
                'files_processed': len(language_results.get('files_analyzed', [])),
                'languages_detected': list(language_results.get('languages', {}).keys()),
                'total_patterns': sum(len(lang_data.get('patterns', [])) for lang_data in language_results.get('languages', {}).values()),
                'processing_time': analytics_report.get('total_runtime_seconds', 0)
            })

            # Phase 3: Content Analysis and Pattern Recognition
            analysis_results = self._run_content_analysis(self.source_dir)

            # Phase 4: Generate Artifacts
            artifacts = self._generate_artifacts(analysis_results, output_dir)

            # Phase 5: Generate Reports
            reports = self._generate_reports(artifacts, output_dir, security_results)

            # Phase 6: Generate Fuzzers (if requested)
            if self.config.get('generate_fuzzers', False):
                fuzzer_results = self._generate_fuzzers(analysis_results, output_dir)
                artifacts.update(fuzzer_results)

            # Final analytics
            self.analytics_logger.stop_performance_tracking()
            analytics_report = self.analytics_logger.generate_analytics_report() or {'total_runtime_seconds': 0, 'processing_rate_mbps': 0, 'success_rate_percent': 100}

            # Record run completion and generate learning insights
            total_files = len(language_results.get('files_analyzed', []))
            processing_time = analytics_report.get('total_runtime_seconds', 0)
            success_rate = len(artifacts.get('fuzzers', [])) / max(total_files, 1)

            self.analytics_db.update_run_stats(self.run_id, total_files, processing_time, success_rate)

            # Generate learning insights from this run
            self._generate_learning_insights(artifacts, language_results, analytics_report)

            return {
                'success': True,
                'artifacts': artifacts,
                'reports': reports,
                'analytics': analytics_report,
                'language_results': language_results,
                'security_results': security_results,
                'risk_assessment': self.risk_assessment,
                'run_id': self.run_id
            }

        except Exception as e:
            self.analytics_logger.log_event('CORE_ENGINE_ERROR', f'Expansion failed: {e}')
            return {
                'success': False,
                'error': str(e)
            }
        finally:
            self.parallel_processor.shutdown()

    def _run_security_analysis(self, source_dir: Path) -> Dict:
        """Run security analysis using available security plugins"""
        print("ðŸ”’ Running security analysis...")

        security_plugins = self.plugin_manager.get_security_plugins()
        if not security_plugins:
            print("âš ï¸  No security plugins available")
            return {}

        # Find source files for analysis
        source_files = []
        for ext in ['*.go', '*.js', '*.java', '*.cpp', '*.cc', '*.c', '*.rs', '*.py']:
            if self.scoped_fs:
                source_files.extend(self.scoped_fs.glob(f"**/{ext}"))
            else:
                source_files.extend(list(self.source_dir.glob(f"**/{ext}")))

        security_results = {
            'vulnerabilities_found': 0,
            'risk_level': 'LOW',
            'findings': [],
            'recommendations': []
        }

        for plugin in security_plugins:
            try:
                result = plugin.scan_vulnerabilities(source_files[:100])  # Limit for performance
                if result.success:
                    findings = result.data.get('findings', [])
                    security_results['findings'].extend(findings)
                    security_results['vulnerabilities_found'] += len(findings)

                    # Update risk level
                    plugin_risk = result.data.get('risk_level', 'LOW')
                    if plugin_risk == 'HIGH' or security_results['risk_level'] == 'HIGH':
                        security_results['risk_level'] = 'HIGH'
                    elif plugin_risk == 'MEDIUM' and security_results['risk_level'] == 'LOW':
                        security_results['risk_level'] = 'MEDIUM'

                    print(f"ðŸ”’ Security plugin {plugin.name}: {len(findings)} findings")
            except Exception as e:
                print(f"âŒ Security plugin {plugin.name} failed: {e}")

        # Generate risk assessment
        self.risk_assessment = {
            'total_vulnerabilities': security_results['vulnerabilities_found'],
            'risk_level': security_results['risk_level'],
            'analysis_timestamp': datetime.now().isoformat(),
            'files_analyzed': len(source_files)
        }

        return security_results

    def _run_language_analysis(self, source_dir: Path) -> Dict:
        """Analyze all source files and detect languages with enhanced accuracy"""
        print("ðŸ” Running enhanced language analysis...")

        source_files = []
        for ext in ['*.go', '*.js', '*.java', '*.cpp', '*.cc', '*.cxx', '*.c', '*.rs', '*.py', '*.ts', '*.jsx', '*.tsx']:
            if self.scoped_fs:
                source_files.extend(self.scoped_fs.glob(f"**/{ext}"))
            else:
                source_files.extend(list(self.source_dir.glob(f"**/{ext}")))

        if not source_files:
            return {'languages': {}, 'file_count': 0}

        # Create language detection tasks with confidence scoring
        tasks = []
        for file_path in source_files[:200]:  # Increased limit for better analysis
            plugin = self.plugin_manager.get_language_plugin_for_file(file_path)
            if plugin:
                tasks.append(ParallelTask(
                    task_type='language_analysis',
                    task_id=f'lang_{file_path.name}',
                    data={'file_path': file_path, 'plugin': plugin}
                ))

        # Submit tasks
        futures = [self.parallel_processor.submit_task(task, 'process') for task in tasks]

        # Collect results with confidence weighting
        language_stats = {}
        confidence_scores = {}

        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                if result and 'language' in result:
                    lang = result['language']
                    confidence = result.get('confidence', 1.0)

                    if lang not in language_stats:
                        language_stats[lang] = 0
                        confidence_scores[lang] = []

                    language_stats[lang] += 1
                    confidence_scores[lang].append(confidence)
            except Exception as e:
                self.analytics_logger.log_event('LANGUAGE_ANALYSIS_ERROR', f'Task failed: {e}')

        # Calculate average confidence per language
        avg_confidence = {}
        for lang, scores in confidence_scores.items():
            avg_confidence[lang] = sum(scores) / len(scores) if scores else 0.0

        return {
            'languages': language_stats,
            'confidence_scores': avg_confidence,
            'file_count': len(source_files),
            'analyzed_count': len(tasks)
        }

    def _run_content_analysis(self, source_dir: Path) -> Dict:
        """Analyze content patterns and extract useful information with ML-like insights"""
        print("ðŸ”Ž Running advanced content analysis...")

        analysis_results = {
            'keywords': set(),
            'functions': set(),
            'patterns': set(),
            'fuzz_targets': [],
            'complexity_metrics': {},
            'api_endpoints': [],
            'data_structures': []
        }

        # Use analysis plugins if available
        for plugin in self.plugin_manager.analysis_plugins.values():
            if plugin.analysis_type == 'content':
                try:
                    result = plugin.analyze({'source_dir': source_dir})
                    if result.success:
                        # Merge results intelligently
                        for key, value in result.data.items():
                            if key in analysis_results:
                                if isinstance(analysis_results[key], set):
                                    analysis_results[key].update(value)
                                elif isinstance(analysis_results[key], list):
                                    analysis_results[key].extend(value)
                                elif isinstance(analysis_results[key], dict):
                                    analysis_results[key].update(value)
                except Exception as e:
                    print(f"âŒ Content analysis plugin failed: {e}")

        # Enhanced pattern recognition
        if self.scoped_fs:
            source_files = list(self.scoped_fs.glob("**/*.py")) + list(self.scoped_fs.glob("**/*.go")) + list(self.scoped_fs.glob("**/*.js"))
        else:
            source_files = list(self.source_dir.glob("**/*.py")) + list(self.source_dir.glob("**/*.go")) + list(self.source_dir.glob("**/*.js"))

        for file_path in source_files[:50]:  # Analyze subset for performance
            try:
                if self.scoped_fs:
                    content = self.scoped_fs.read_text(file_path)
                else:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')

                # Extract function signatures
                func_patterns = [
                    r'def\s+(\w+)\s*\(',  # Python
                    r'func\s+(\w+)\s*\(',  # Go
                    r'function\s+(\w+)\s*\(',  # JavaScript
                ]

                for pattern in func_patterns:
                    matches = re.findall(pattern, content)
                    analysis_results['functions'].update(matches[:10])  # Limit per file

                # Look for API endpoints
                api_patterns = [
                    r'@app\.route\(["\']([^"\']+)["\']',  # Flask
                    r'router\.[get|post|put|delete]+\(["\']([^"\']+)["\']',  # Express
                ]

                for pattern in api_patterns:
                    matches = re.findall(pattern, content)
                    analysis_results['api_endpoints'].extend(matches)

            except Exception as e:
                continue

        # Convert sets to lists for JSON serialization
        analysis_results['keywords'] = list(analysis_results['keywords'])
        analysis_results['functions'] = list(analysis_results['functions'])
        analysis_results['patterns'] = list(analysis_results['patterns'])

        return analysis_results

    def _generate_artifacts(self, analysis_results: Dict, output_dir: Path) -> Dict:
        """Generate fuzzing artifacts with enhanced intelligence"""
        print("ðŸ”§ Generating intelligent fuzzing artifacts...")

        artifacts = {
            'seeds_generated': 0,
            'dictionaries_generated': 0,
            'options_generated': 0,
            'harnesses_generated': 0,
            'total_size': 0,
            'artifact_quality_score': 0.0
        }

        # Generate enhanced smart dictionary
        dict_path = output_dir / "intelligent_dictionary.txt"
        dictionary_entries = set()

        # Add function names
        if analysis_results.get('functions'):
            dictionary_entries.update(analysis_results['functions'])

        # Add API endpoints
        if analysis_results.get('api_endpoints'):
            for endpoint in analysis_results['api_endpoints']:
                # Extract path components
                parts = endpoint.strip('/').split('/')
                dictionary_entries.update(parts)

        # Add keywords
        if analysis_results.get('keywords'):
            dictionary_entries.update(analysis_results['keywords'])

        if dictionary_entries:
            with open(dict_path, 'w') as f:
                for entry in sorted(dictionary_entries):
                    if len(entry) > 2 and entry.isalnum():  # Quality filter
                        f.write(f"{entry}\n")
            artifacts['dictionaries_generated'] = 1
            artifacts['total_size'] += dict_path.stat().st_size
            artifacts['artifact_quality_score'] += 0.3
        else:
            # Create minimal dictionary
            dict_path.touch()
            artifacts['dictionaries_generated'] = 1

        # Generate intelligent seed corpus
        seeds_dir = output_dir / "intelligent_seeds"
        seeds_dir.mkdir(exist_ok=True)

        # Generate seeds based on discovered patterns
        seed_count = 0
        if analysis_results.get('api_endpoints'):
            for i, endpoint in enumerate(analysis_results['api_endpoints'][:10]):
                seed_file = seeds_dir / f"api_seed_{i}.txt"
                with open(seed_file, 'w') as f:
                    f.write(f"GET {endpoint} HTTP/1.1\nHost: example.com\n\n")
                seed_count += 1

        artifacts['seeds_generated'] = seed_count
        if seed_count > 0:
            artifacts['artifact_quality_score'] += 0.4

        # Generate enhanced options file
        options_path = output_dir / "intelligent.options"
        with open(options_path, 'w') as f:
            f.write("# Intelligent fuzzing options\n")
            f.write(f"dict={dict_path}\n")
            f.write("max_len=65536\n")
            f.write("timeout=30\n")

            # Add intelligent options based on analysis
            if analysis_results.get('complexity_metrics'):
                f.write("# Complexity-based optimizations\n")
                f.write("runs=1000000\n")

            if len(analysis_results.get('functions', [])) > 50:
                f.write("# High function count - extended fuzzing\n")
                f.write("jobs=4\n")

        artifacts['options_generated'] = 1
        artifacts['total_size'] += options_path.stat().st_size
        artifacts['artifact_quality_score'] += 0.3

        # Normalize quality score
        artifacts['artifact_quality_score'] = min(artifacts['artifact_quality_score'], 1.0)

        return artifacts

    def _generate_reports(self, artifacts: Dict, output_dir: Path, security_results: Dict = None) -> Dict:
        """Generate comprehensive reports including security findings"""
        print("ðŸ“Š Generating comprehensive reports...")

        reports = {}

        # Prepare enhanced report data
        report_data = {
            'artifacts': artifacts,
            'security': security_results or {},
            'timestamp': datetime.now().isoformat(),
            'quality_metrics': {
                'artifact_quality': artifacts.get('artifact_quality_score', 0.0),
                'security_score': 1.0 - (security_results.get('vulnerabilities_found', 0) * 0.1) if security_results else 1.0
            }
        }

        # Generate reports using reporting plugins
        for format_name, plugin in self.plugin_manager.reporting_plugins.items():
            if self.config.get(f'enable_{format_name}_reports', True):
                try:
                    output_path = output_dir / f"comprehensive_report.{format_name}"
                    result = plugin.generate_report(
                        report_data,
                        {},
                        [],
                        output_path
                    )

                    if result.success:
                        reports[format_name] = str(output_path)
                        print(f"âœ… Generated {format_name.upper()} report: {output_path}")

                except Exception as e:
                    print(f"âŒ Failed to generate {format_name} report: {e}")

        # Generate security report if findings exist
        if security_results and security_results.get('vulnerabilities_found', 0) > 0:
            security_report_path = output_dir / "security_findings.json"
            with open(security_report_path, 'w') as f:
                json.dump(security_results, f, indent=2, default=str)
            reports['security'] = str(security_report_path)
            print(f"ðŸ”’ Generated security report: {security_report_path}")

        return reports

    def _generate_fuzzers(self, analysis_results: Dict, output_dir: Path) -> Dict:
        """Generate intelligent fuzzing harnesses with enhanced templates"""
        print("ðŸ§¬ Generating intelligent fuzzing harnesses...")

        fuzzer_artifacts = {
            'harnesses_generated': 0,
            'languages_covered': set(),
            'generated_harness_files': [],
            'harness_quality_score': 0.0
        }

        # Enhanced fuzzer generation based on discovered functions and APIs
        functions = analysis_results.get('functions', [])
        api_endpoints = analysis_results.get('api_endpoints', [])

        # Generate API fuzzers
        if api_endpoints:
            api_fuzzer_dir = output_dir / "api_fuzzers"
            api_fuzzer_dir.mkdir(exist_ok=True)

            for i, endpoint in enumerate(api_endpoints[:5]):  # Quality over quantity
                fuzzer_file = api_fuzzer_dir / f"fuzz_api_{i}.py"
                with open(fuzzer_file, 'w') as f:
                    f.write(f'''#!/usr/bin/env python3
"""
Auto-generated API fuzzer for endpoint: {endpoint}
"""
import atheris
import sys
import requests

def TestOneInput(data):
    try:
        # Fuzz the API endpoint
        url = f"http://localhost:8080{endpoint}"
        response = requests.post(url, data=data, timeout=1)
        return response.status_code
    except:
        pass

def main():
    atheris.instrument_all()
    atheris.Setup(sys.argv, TestOneInput)
    atheris.Fuzz()

if __name__ == "__main__":
    main()
''')
                fuzzer_artifacts['harnesses_generated'] += 1
                fuzzer_artifacts['generated_harness_files'].append(str(fuzzer_file))
                fuzzer_artifacts['languages_covered'].add('python')

        # Generate function-based fuzzers
        if functions:
            func_fuzzer_dir = output_dir / "function_fuzzers"
            func_fuzzer_dir.mkdir(exist_ok=True)

            for i, func_name in enumerate(list(functions)[:3]):  # Quality over quantity
                fuzzer_file = func_fuzzer_dir / f"fuzz_{func_name}.go"
                with open(fuzzer_file, 'w') as f:
                    f.write(f'''package main

import (
    "testing"
)

// Auto-generated fuzzer for function: {func_name}
func Fuzz{func_name.capitalize()}(f *testing.F) {{
    f.Add([]byte("test input"))
    f.Fuzz(func(t *testing.T, data []byte) {{
        // Call the target function with fuzzed data
        // {func_name}(string(data))
    }})
}}
''')
                fuzzer_artifacts['harnesses_generated'] += 1
                fuzzer_artifacts['generated_harness_files'].append(str(fuzzer_file))
                fuzzer_artifacts['languages_covered'].add('go')

        # Calculate harness quality score
        if fuzzer_artifacts['harnesses_generated'] > 0:
            quality_factors = [
                len(fuzzer_artifacts['languages_covered']) * 0.3,  # Language diversity
                min(fuzzer_artifacts['harnesses_generated'] / 10.0, 0.5),  # Quantity (capped)
                0.2 if api_endpoints else 0.0,  # API coverage
            ]
            fuzzer_artifacts['harness_quality_score'] = sum(quality_factors)

        fuzzer_artifacts['languages_covered'] = list(fuzzer_artifacts['languages_covered'])

        if fuzzer_artifacts['harnesses_generated'] > 0:
            print(f"âœ… Generated {fuzzer_artifacts['harnesses_generated']} intelligent fuzzing harnesses")
            print(f"ðŸŽ¯ Languages covered: {', '.join(fuzzer_artifacts['languages_covered'])}")
            print(f"ðŸ“Š Harness quality score: {fuzzer_artifacts['harness_quality_score']:.2f}")

        return fuzzer_artifacts

# Phase 2: Parallel Processing Architecture
@dataclass
class ParallelTask:
    """Represents a parallelizable task"""
    task_type: str  # 'file_analysis', 'language_detection', 'build_execution'
    task_id: str
    data: Dict
    priority: int = 0

class ParallelProcessor:
    """Advanced parallel processing system for maximum performance"""

    def __init__(self, max_workers: int, analytics_logger: 'AnalyticsLogger'):
        self.max_workers = max_workers
        self.analytics_logger = analytics_logger

        # Thread pool for I/O-bound tasks (file operations, subprocess calls)
        self.thread_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers * 2,  # More threads for I/O
            thread_name_prefix="FuzzExpansion-IO"
        )

        # Process pool for CPU-bound tasks (analysis, detection)
        self.process_executor = concurrent.futures.ProcessPoolExecutor(
            max_workers=max_workers,
            max_tasks_per_child=50  # Restart processes periodically
        )

        self.tasks_completed = 0
        self.tasks_failed = 0

    def submit_task(self, task: ParallelTask, executor_type: str = 'thread'):
        """Submit a task to the appropriate executor"""
        executor = self.thread_executor if executor_type == 'thread' else self.process_executor

        future = executor.submit(self._execute_task, task)
        future.add_done_callback(lambda f: self._task_complete_callback(f, task))
        return future

    def _execute_task(self, task: ParallelTask):
        """Execute a single task"""
        try:
            if task.task_type == 'file_analysis':
                return self._analyze_files_task(task.data)
            elif task.task_type == 'language_detection':
                return self._detect_languages_task(task.data)
            elif task.task_type == 'intelligent_language_detection':
                return self._intelligent_language_detection_task(task.data)
            elif task.task_type == 'build_execution':
                return self._build_execution_task(task.data)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
        except Exception as e:
            self.analytics_logger.log_event('TASK_FAILED', f'Task {task.task_id} failed: {e}', error=str(e))
            raise

    def _analyze_files_task(self, data: Dict) -> Dict:
        """CPU-bound task: Analyze files in parallel"""
        file_paths = data['file_paths']
        analysis_type = data['analysis_type']

        results = {}
        for file_path in file_paths:
            try:
                if analysis_type == 'size':
                    results[str(file_path)] = file_path.stat().st_size
                elif analysis_type == 'content':
                    # Analyze file content for fuzz targets
                    content = file_path.read_text() if file_path.is_file() else ""
                    results[str(file_path)] = self._analyze_file_content(content)
            except Exception as e:
                results[str(file_path)] = f"Error: {e}"

        return results

    def _detect_languages_task(self, data: Dict) -> Dict:
        """CPU-bound task: Detect programming languages"""
        file_paths = data['file_paths']

        results = {}
        for file_path in file_paths:
            try:
                results[str(file_path)] = self._detect_file_language(file_path)
            except Exception as e:
                results[str(file_path)] = f"Error: {e}"

        return results

    def _intelligent_language_detection_task(self, data: Dict) -> List[Dict]:
        """CPU-bound task: Intelligent language detection with confidence scoring"""
        file_paths = data['file_paths']
        detector = data['detector']

        results = []
        for file_path in file_paths:
            try:
                detection_result = detector.detect_language_advanced(file_path)
                results.append({
                    'file_path': str(file_path),
                    **detection_result
                })
            except Exception as e:
                results.append({
                    'file_path': str(file_path),
                    'language': 'error',
                    'confidence': 0.0,
                    'error': str(e)
                })

        return results

    def _build_execution_task(self, data: Dict) -> Dict:
        """I/O-bound task: Execute build commands"""
        command = data['command']
        cwd = data['cwd']
        timeout = data.get('timeout', 300)

        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout,
                check=True
            )
            return {
                'success': True,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'returncode': result.returncode
            }
        except subprocess.TimeoutExpired:
            return {'success': False, 'error': 'Command timed out'}
        except subprocess.CalledProcessError as e:
            return {
                'success': False,
                'error': f'Command failed: {e}',
                'returncode': e.returncode,
                'stderr': e.stderr
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _analyze_file_content(self, content: str) -> Dict:
        """Analyze file content for fuzzing patterns"""
        analysis = {
            'is_fuzz_target': False,
            'language': None,
            'fuzz_function_count': 0,
            'has_imports': False
        }

        # Go fuzz target patterns
        if 'func Fuzz' in content:
            analysis['is_fuzz_target'] = True
            analysis['language'] = 'go'
            analysis['fuzz_function_count'] = content.count('func Fuzz')

        # JavaScript fuzz target patterns
        elif 'function fuzz' in content or 'exports.fuzz' in content:
            analysis['is_fuzz_target'] = True
            analysis['language'] = 'javascript'
            analysis['fuzz_function_count'] = content.count('function fuzz') + content.count('exports.fuzz')

        # Java fuzz target patterns
        elif '@FuzzTest' in content or 'fuzzerTestOneInput' in content:
            analysis['is_fuzz_target'] = True
            analysis['language'] = 'java'
            analysis['fuzz_function_count'] = content.count('@FuzzTest') + content.count('fuzzerTestOneInput')

        # C/C++ fuzz target patterns
        elif 'LLVMFuzzerTestOneInput' in content:
            analysis['is_fuzz_target'] = True
            analysis['language'] = 'cpp'
            analysis['fuzz_function_count'] = content.count('LLVMFuzzerTestOneInput')

        # Python fuzz target patterns
        elif 'def fuzz' in content:
            analysis['is_fuzz_target'] = True
            analysis['language'] = 'python'
            analysis['fuzz_function_count'] = content.count('def fuzz')

        # Check for imports
        analysis['has_imports'] = any(imp in content for imp in ['import', '#include', 'require(', 'from '])

        return analysis

    def _detect_file_language(self, file_path: Path) -> str:
        """Detect programming language of a file"""
        if not file_path.is_file():
            return 'unknown'

        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')

            # Language detection patterns
            if 'package main' in content and 'func ' in content:
                return 'go'
            elif 'function ' in content and ('require(' in content or 'module.exports' in content):
                return 'javascript'
            elif 'public class' in content and 'import java.' in content:
                return 'java'
            elif '#include' in content and ('int main(' in content or 'LLVMFuzzerTestOneInput' in content):
                return 'cpp'
            elif 'def ' in content and ('import ' in content or 'from ' in content):
                return 'python'
            elif 'fn ' in content and 'use ' in content:
                return 'rust'
            else:
                return 'unknown'
        except Exception:
            return 'unknown'

    def _task_complete_callback(self, future: concurrent.futures.Future, task: ParallelTask):
        """Callback when a task completes"""
        self.tasks_completed += 1

        if future.exception():
            self.tasks_failed += 1
            self.analytics_logger.log_event('TASK_EXCEPTION', f'Task {task.task_id} raised exception', error=str(future.exception()))
        else:
            result = future.result()
            self.analytics_logger.log_event('TASK_COMPLETED', f'Task {task.task_id} completed successfully', result_count=len(result) if isinstance(result, dict) else 1)

    def shutdown(self, wait: bool = True):
        """Shutdown all executors"""
        self.thread_executor.shutdown(wait=wait)
        self.process_executor.shutdown(wait=wait)

        self.analytics_logger.log_event('PARALLEL_SHUTDOWN', f'Parallel processing shutdown. Completed: {self.tasks_completed}, Failed: {self.tasks_failed}')

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()

# Advanced Analytics & Monitoring Structures
@dataclass
class PerformanceMetrics:
    """Track performance metrics throughout the expansion process"""
    start_time: float = 0.0
    end_time: float = 0.0
    cpu_percent_start: float = 0.0
    memory_mb_start: float = 0.0
    total_commands_executed: int = 0
    failed_commands: int = 0
    files_processed: int = 0
    bytes_processed: int = 0

    @property
    def duration_seconds(self) -> float:
        return self.end_time - self.start_time if self.end_time > 0 else 0

    @property
    def processing_rate_mbps(self) -> float:
        if self.duration_seconds > 0 and self.bytes_processed > 0:
            return (self.bytes_processed / 1_000_000) / self.duration_seconds
        return 0.0

    @property
    def success_rate(self) -> float:
        if self.total_commands_executed > 0:
            return ((self.total_commands_executed - self.failed_commands) / self.total_commands_executed) * 100
        return 100.0

@dataclass
class ExpansionStats:
    """Comprehensive statistics tracking"""
    fuzz_targets: int = 0
    seed_corpora: int = 0
    dictionaries: int = 0
    options_files: int = 0
    total_size_mb: float = 0.0
    languages_detected: List[str] = None
    build_success: bool = False
    errors_encountered: List[str] = None
    warnings_generated: List[str] = None

    def __post_init__(self):
        if self.languages_detected is None:
            self.languages_detected = []
        if self.errors_encountered is None:
            self.errors_encountered = []
        if self.warnings_generated is None:
            self.warnings_generated = []

class AnalyticsLogger:
    """Advanced logging and analytics system"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.setup_logging()

        # Analytics data
        self.performance_metrics = PerformanceMetrics()
        self.expansion_stats = ExpansionStats()
        self.timeline_events = []
        self.resource_usage_history = []

    def setup_logging(self):
        """Setup structured logging"""
        self.logger = logging.getLogger('RapidExpander')
        self.logger.setLevel(logging.INFO)

        # File handler
        fh = logging.FileHandler(self.log_file)
        fh.setLevel(logging.DEBUG)

        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)

        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        self.logger.addHandler(fh)
        self.logger.addHandler(ch)

    def log_event(self, event_type: str, message: str, **kwargs):
        """Log a timeline event with metadata"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'type': event_type,
            'message': message,
            'metadata': kwargs
        }
        self.timeline_events.append(event)
        self.logger.info(f"[{event_type}] {message}")

    def track_resource_usage(self):
        """Track current resource usage (with graceful fallback)"""
        usage = {'timestamp': datetime.now().isoformat()}

        if HAS_PSUTIL:
            try:
                usage.update({
                    'cpu_percent': psutil.cpu_percent(interval=0.1),
                    'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                    'memory_percent': psutil.virtual_memory().percent
                })
            except Exception:
                # Fallback values if psutil fails
                usage.update({
                    'cpu_percent': 0.0,
                    'memory_mb': 0.0,
                    'memory_percent': 0.0
                })
        else:
            # Default values when psutil not available
            usage.update({
                'cpu_percent': 0.0,
                'memory_mb': 0.0,
                'memory_percent': 0.0
            })

        self.resource_usage_history.append(usage)
        return usage

    def start_performance_tracking(self):
        """Start performance tracking"""
        self.performance_metrics.start_time = time.time()

        if HAS_PSUTIL:
            try:
                self.performance_metrics.cpu_percent_start = psutil.cpu_percent()
                self.performance_metrics.memory_mb_start = psutil.Process().memory_info().rss / 1024 / 1024
            except Exception:
                self.performance_metrics.cpu_percent_start = 0.0
                self.performance_metrics.memory_mb_start = 0.0
        else:
            self.performance_metrics.cpu_percent_start = 0.0
            self.performance_metrics.memory_mb_start = 0.0

        self.log_event('START', 'Performance tracking started')

    def stop_performance_tracking(self):
        """Stop performance tracking and calculate final metrics"""
        self.performance_metrics.end_time = time.time()
        self.log_event('END', f'Performance tracking completed after {self.performance_metrics.duration_seconds:.2f}s')

    def generate_analytics_report(self) -> Dict:
        """Generate comprehensive analytics report"""
        return {
            'performance_metrics': asdict(self.performance_metrics),
            'expansion_stats': asdict(self.expansion_stats),
            'timeline_events': self.timeline_events,
            'resource_usage_history': self.resource_usage_history,
            'summary': {
                'duration_seconds': self.performance_metrics.duration_seconds,
                'processing_rate_mbps': self.performance_metrics.processing_rate_mbps,
                'success_rate_percent': self.performance_metrics.success_rate,
                'peak_memory_mb': max([u['memory_mb'] for u in self.resource_usage_history], default=0),
                'avg_cpu_percent': sum([u['cpu_percent'] for u in self.resource_usage_history]) / len(self.resource_usage_history) if self.resource_usage_history else 0
            }
        }

@contextmanager
def timer_context(name: str, logger: AnalyticsLogger):
    """Context manager for timing operations"""
    start_time = time.time()
    logger.log_event('TIMER_START', f'Starting {name}')

    try:
        yield
    finally:
        duration = time.time() - start_time
        logger.log_event('TIMER_END', f'Completed {name} in {duration:.2f}s', duration=duration)

def with_performance_tracking(func):
    """Decorator to add performance tracking to methods"""
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        if hasattr(self, 'analytics_logger'):
            method_name = func.__name__
            with timer_context(method_name, self.analytics_logger):
                return func(self, *args, **kwargs)
        return func(self, *args, **kwargs)
    return wrapper

class RapidExpander:
    def __init__(self, project_name="gemini_cli", oss_fuzz_root=None):
        self.project_name = project_name
        self.oss_fuzz_root = Path(oss_fuzz_root or self.find_oss_fuzz_root())
        self.project_dir = self.oss_fuzz_root / "projects" / project_name
        self.build_script = self.project_dir / "build.sh"

        # Initialize analytics and logging system
        log_dir = self.project_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f"rapid_expansion_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        self.analytics_logger = AnalyticsLogger(log_file)

        # Legacy stats for backward compatibility (will migrate to analytics system)
        self.stats = {
            'fuzz_targets': 0,
            'seed_corpora': 0,
            'dictionaries': 0,
            'options_files': 0,
            'total_size_mb': 0.0,
            'languages': {},
            'performance_metrics': {},
            'recommendations': []
        }

        # Enhanced visual feedback system
        self.progress_chars = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
        self.colors = {
            'success': '\033[92m',
            'warning': '\033[93m',
            'error': '\033[91m',
            'info': '\033[94m',
            'reset': '\033[0m',
            'bold': '\033[1m',
            'magenta': '\033[95m',
            'cyan': '\033[96m'
        }

        # Initialize parallel processor
        self.parallel_processor = ParallelProcessor(
            max_workers=self.config['max_workers'],
            analytics_logger=self.analytics_logger
        )

        # Initialize Phase 3: Intelligent Automation
        self.intelligent_detector = IntelligentDetector()
        self.auto_optimizer = AutoOptimizer(self.analytics_logger)
        self.smart_dictionary_generator = SmartDictionaryGenerator()
        self.intelligence_metrics = IntelligenceMetrics()

        # Initialize Phase 4: Professional Reporting
        self.report_generator = ProfessionalReportGenerator(self.project_name, self.project_dir)

        # Configuration management
        self.config = {
            'max_workers': min(4, os.cpu_count() or 2),
            'enable_parallel_processing': True,
            'enable_resource_monitoring': True,
            'log_level': 'INFO',
            'progress_bar_enabled': True,
            'analytics_enabled': True,
            'parallel_file_analysis': True,
            'parallel_language_detection': True,
            'parallel_build_execution': True,
            # Phase 3: Intelligent Automation
            'enable_intelligent_detection': True,
            'enable_auto_optimization': True,
            'enable_smart_dictionary': True,
            'language_confidence_threshold': 0.7,
            'fuzz_target_probability_threshold': 0.6,
            # Phase 4: Professional Reporting
            'enable_html_reports': True,
            'enable_pdf_reports': True,
            'enable_json_reports': True,
            'enable_visualizations': True
        }

    def find_oss_fuzz_root(self):
        """Find the OSS-Fuzz root directory"""
        current = Path.cwd()
        while current.parent != current:
            if (current / "infra" / "helper.py").exists():
                return current
            current = current.parent
        return Path("/tmp/oss-fuzz")  # fallback

    @with_performance_tracking
    def run_command(self, cmd, cwd=None, check=True, capture_output=False):
        """Enhanced command execution with analytics tracking"""
        self.analytics_logger.performance_metrics.total_commands_executed += 1

        try:
            self.analytics_logger.log_event('CMD_START', f'Executing: {cmd}')

            # Track resource usage before command
            if self.config['enable_resource_monitoring']:
                self.analytics_logger.track_resource_usage()

            result = subprocess.run(
                cmd,
                shell=True,
                cwd=cwd or self.project_dir,
                capture_output=capture_output,
                text=True,
                check=check
            )

            self.analytics_logger.log_event('CMD_SUCCESS', f'Command completed successfully: {cmd}')
            return result

        except subprocess.CalledProcessError as e:
            self.analytics_logger.performance_metrics.failed_commands += 1
            error_msg = f"Command failed: {cmd} - {e}"
            self.analytics_logger.expansion_stats.errors_encountered.append(error_msg)
            self.analytics_logger.log_event('CMD_FAILED', error_msg, error=str(e))

            if not check:
                return e
            sys.exit(1)

    def show_progress_bar(self, desc: str, total: int = None):
        """Show animated progress bar (with fallback)"""
        if not self.config['progress_bar_enabled']:
            return None

        if HAS_TQDM:
            return tqdm(
                total=total,
                desc=desc,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
                colour='green',
                dynamic_ncols=True
            )
        else:
            # Simple fallback progress indicator
            class SimpleProgress:
                def __init__(self, desc, total):
                    self.desc = desc
                    self.total = total
                    self.current = 0

                def update(self, n=1):
                    self.current += n
                    if self.total:
                        progress = int((self.current / self.total) * 50)
                        bar = "â–ˆ" * progress + "â–‘" * (50 - progress)
                        print(f"\r{self.desc}: [{bar}] {self.current}/{self.total}", end="", flush=True)

                def close(self):
                    print()  # New line

            return SimpleProgress(desc, total)

    @with_performance_tracking
    def setup_environment(self):
        """Setup environment for rapid expansion with enhanced analytics"""
        self.analytics_logger.log_event('ENV_SETUP', 'Starting environment setup')

        # Create necessary directories
        build_dir = self.oss_fuzz_root / "build"
        build_dir.mkdir(exist_ok=True)

        out_dir = build_dir / "out" / self.project_name
        out_dir.mkdir(parents=True, exist_ok=True)

        work_dir = build_dir / "work" / self.project_name
        work_dir.mkdir(parents=True, exist_ok=True)

        # Set environment variables
        env_vars = {
            "OUT": str(out_dir),
            "SRC": str(self.oss_fuzz_root / "projects" / self.project_name),
            "WORK": str(work_dir),
            "PROJECT_NAME": self.project_name,
            "RAPID_EXPANSION": "true",
            "DEBUG": "true"
        }

        for key, value in env_vars.items():
            os.environ[key] = value

        self.analytics_logger.log_event('ENV_SETUP', 'Environment setup completed', env_vars=env_vars)
        print(f"âœ… Environment setup complete")
        return env_vars

    def python_helper_check(self):
        """Run Python helper validation"""
        print("ðŸ” Running Python helper validation...")

        try:
            result = self.run_command(
                f"cd {self.oss_fuzz_root} && python3 infra/helper.py check_build {self.project_name}",
                capture_output=True
            )
            print("âœ… Python helper validation passed")
            return True
        except subprocess.CalledProcessError:
            print("âš ï¸  Python helper validation failed, continuing...")
            return False

    def enhanced_build_script(self):
        """Run the enhanced build script with rapid expansion"""
        print("ðŸš€ Running enhanced build script with rapid expansion...")

        if not self.build_script.exists():
            print(f"âŒ Build script not found: {self.build_script}")
            return False

        try:
            result = self.run_command(f"bash {self.build_script} --rapid")
            print("âœ… Enhanced build script completed successfully")
            return True
        except subprocess.CalledProcessError:
            print("âŒ Enhanced build script failed")
            return False

    @with_performance_tracking
    def analyze_results(self):
        """Analyze the results of the rapid expansion with parallel processing"""
        self.analytics_logger.log_event('ANALYSIS_START', 'Starting parallel result analysis')

        out_dir = self.oss_fuzz_root / "build" / "out" / self.project_name
        if not out_dir.exists():
            error_msg = f"Output directory not found: {out_dir}"
            self.analytics_logger.expansion_stats.errors_encountered.append(error_msg)
            self.analytics_logger.log_event('ANALYSIS_ERROR', error_msg)
            print("âŒ Output directory not found")
            return {}

        # Use parallel processing for file discovery and analysis
        if self.config['enable_parallel_processing'] and self.config['parallel_file_analysis']:
            return self._analyze_results_parallel(out_dir)
        else:
            return self._analyze_results_sequential(out_dir)

    def _analyze_results_parallel(self, out_dir: Path) -> Dict:
        """Parallel analysis using the parallel processor"""
        self.analytics_logger.log_event('PARALLEL_ANALYSIS_START', 'Starting parallel file analysis')

        # Create tasks for different file types
        tasks = []
        task_id = 0

        # Task for finding all files (I/O bound)
        all_files = list(out_dir.rglob("*"))
        chunk_size = max(10, len(all_files) // (self.config['max_workers'] * 4))

        for i in range(0, len(all_files), chunk_size):
            chunk = all_files[i:i + chunk_size]
            tasks.append(ParallelTask(
                task_type='file_analysis',
                task_id=f'file_analysis_{task_id}',
                data={'file_paths': chunk, 'analysis_type': 'content'},
                priority=1
            ))
            task_id += 1

        # Submit tasks to thread executor (I/O bound)
        futures = [self.parallel_processor.submit_task(task, 'thread') for task in tasks]

        # Progress tracking
        progress = self.show_progress_bar("Parallel file analysis", len(futures))
        completed = 0

        # Collect results
        analysis_results = {}
        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                analysis_results.update(result)
                completed += 1
                if progress:
                    progress.update(1)
            except Exception as e:
                self.analytics_logger.log_event('ANALYSIS_TASK_ERROR', f'Task failed: {e}')

        if progress:
            progress.close()

        # Process analysis results
        return self._process_analysis_results(analysis_results, out_dir)

    def _analyze_results_sequential(self, out_dir: Path) -> Dict:
        """Sequential analysis for fallback or when parallel processing is disabled"""
        progress = self.show_progress_bar("Analyzing output files", 5)

        # Count different types of files
        fuzz_targets = list(out_dir.glob("fuzz_*")) + list(out_dir.glob("*Fuzz*"))
        seed_corpora = list(out_dir.glob("*_seed_corpus.zip"))
        dictionaries = list(out_dir.glob("*.dict"))
        options_files = list(out_dir.glob("*.options"))

        if progress:
            progress.update(1)

        # Calculate total size
        total_size_mb = self.get_directory_size_mb(out_dir)
        if progress:
            progress.update(1)

        # Update analytics stats
        self.analytics_logger.expansion_stats.fuzz_targets = len(fuzz_targets)
        self.analytics_logger.expansion_stats.seed_corpora = len(seed_corpora)
        self.analytics_logger.expansion_stats.dictionaries = len(dictionaries)
        self.analytics_logger.expansion_stats.options_files = len(options_files)
        self.analytics_logger.expansion_stats.total_size_mb = total_size_mb

        # Calculate bytes processed for performance metrics
        bytes_processed = 0
        for file_list in [fuzz_targets, seed_corpora, dictionaries, options_files]:
            for file_path in file_list:
                if file_path.is_file():
                    bytes_processed += file_path.stat().st_size
                    self.analytics_logger.performance_metrics.files_processed += 1

        self.analytics_logger.performance_metrics.bytes_processed = bytes_processed

        if progress:
            progress.update(3)
            progress.close()

        results = {
            "fuzz_targets": len(fuzz_targets),
            "seed_corpora": len(seed_corpora),
            "dictionaries": len(dictionaries),
            "options_files": len(options_files),
            "total_size_mb": total_size_mb,
            "total_files_processed": self.analytics_logger.performance_metrics.files_processed,
            "bytes_processed": bytes_processed
        }

        return results

    def _process_analysis_results(self, analysis_results: Dict, out_dir: Path) -> Dict:
        """Process parallel analysis results into final statistics"""
        fuzz_targets = []
        seed_corpora = []
        dictionaries = []
        options_files = []
        total_bytes = 0

        for file_path_str, analysis in analysis_results.items():
            file_path = Path(file_path_str)

            if isinstance(analysis, dict):
                if analysis.get('is_fuzz_target'):
                    fuzz_targets.append(file_path)
                # Categorize by file extension
                if file_path.suffix == '.zip' and 'seed_corpus' in file_path.name:
                    seed_corpora.append(file_path)
                elif file_path.suffix == '.dict':
                    dictionaries.append(file_path)
                elif file_path.suffix == '.options':
                    options_files.append(file_path)

                # Track file size if available
                if 'size' in analysis:
                    total_bytes += analysis['size']
                elif file_path.is_file():
                    total_bytes += file_path.stat().st_size

                self.analytics_logger.performance_metrics.files_processed += 1

        # Update analytics stats
        self.analytics_logger.expansion_stats.fuzz_targets = len(fuzz_targets)
        self.analytics_logger.expansion_stats.seed_corpora = len(seed_corpora)
        self.analytics_logger.expansion_stats.dictionaries = len(dictionaries)
        self.analytics_logger.expansion_stats.options_files = len(options_files)
        self.analytics_logger.expansion_stats.total_size_mb = total_bytes / (1024 * 1024)
        self.analytics_logger.performance_metrics.bytes_processed = total_bytes

        results = {
            "fuzz_targets": len(fuzz_targets),
            "seed_corpora": len(seed_corpora),
            "dictionaries": len(dictionaries),
            "options_files": len(options_files),
            "total_size_mb": total_bytes / (1024 * 1024),
            "total_files_processed": self.analytics_logger.performance_metrics.files_processed,
            "bytes_processed": total_bytes
        }

        # Enhanced output with performance metrics
        print(f"ðŸ“ˆ Parallel Analysis Results:")
        print(f"  ðŸŽ¯ Fuzz targets: {results['fuzz_targets']}")
        print(f"  ðŸ“¦ Seed corpora: {results['seed_corpora']}")
        print(f"  ðŸ“š Dictionaries: {results['dictionaries']}")
        print(f"  âš™ï¸  Options files: {results['options_files']}")
        print(f"  ðŸ’¾ Total size: {results['total_size_mb']:.2f} MB")
        print(f"  ðŸ“ Files processed: {results['total_files_processed']}")
        print(f"  âš¡ Processing rate: {self.analytics_logger.performance_metrics.processing_rate_mbps:.2f} MB/s")
        print(f"  ðŸš€ Parallel efficiency: {len(analysis_results)} files analyzed concurrently")

        self.analytics_logger.log_event('PARALLEL_ANALYSIS_COMPLETE', 'Parallel result analysis completed', results=results)
        return results

    def get_directory_size_mb(self, path):
        """Get directory size in MB"""
        try:
            result = self.run_command(f"du -sm {path}", capture_output=True)
            return float(result.stdout.split()[0])
        except:
            return 0.0

    def create_summary_report(self, results):
        """Create a summary report"""
        report = {
            "project": self.project_name,
            "timestamp": datetime.now().isoformat(),
            "results": results,
            "languages_detected": self.detect_languages(),
            "recommendations": self.generate_recommendations(results)
        }

        report_file = self.project_dir / "rapid_expansion_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"ðŸ“„ Summary report saved to: {report_file}")
        return report

    @with_performance_tracking
    def detect_languages(self):
        """Detect programming languages used in the project with parallel processing"""
        if self.config['enable_parallel_processing'] and self.config['parallel_language_detection']:
            return self._detect_languages_parallel()
        else:
            return self._detect_languages_sequential()

    def _detect_languages_parallel(self):
        """Parallel language detection using the intelligent detector"""
        self.analytics_logger.log_event('PARALLEL_LANGUAGE_DETECTION_START', 'Starting parallel language detection')

        # Find all potential source files
        source_files = []
        for pattern in ["*.go", "*.js", "*.java", "*.cpp", "*.cc", "*.cxx", "*.c", "*.rs", "*.py"]:
            source_files.extend(list(self.project_dir.glob(f"**/{pattern}")))

        if not source_files:
            return []

        # Create tasks for intelligent language detection
        tasks = []
        chunk_size = max(5, len(source_files) // (self.config['max_workers'] * 2))

        for i in range(0, len(source_files), chunk_size):
            chunk = source_files[i:i + chunk_size]
            tasks.append(ParallelTask(
                task_type='intelligent_language_detection',
                task_id=f'lang_detect_{i}',
                data={'file_paths': chunk, 'detector': self.intelligent_detector},
                priority=2
            ))

        # Submit tasks to process executor (CPU bound)
        futures = [self.parallel_processor.submit_task(task, 'process') for task in tasks]

        # Collect results
        language_results = {}
        intelligence_data = []

        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                for item in result:
                    language = item.get('language', 'unknown')
                    confidence = item.get('confidence', 0.0)
                    if language != 'unknown' and confidence >= self.config['language_confidence_threshold']:
                        language_results[language] = language_results.get(language, 0) + 1
                        intelligence_data.append(item)
            except Exception as e:
                self.analytics_logger.log_event('LANGUAGE_DETECTION_ERROR', f'Task failed: {e}')

        detected_languages = list(language_results.keys())

        # Update intelligence metrics
        if intelligence_data:
            avg_confidence = sum(item.get('confidence', 0) for item in intelligence_data) / len(intelligence_data)
            self.intelligence_metrics.language_confidence = avg_confidence

        # Update analytics
        self.analytics_logger.expansion_stats.languages_detected = detected_languages
        self.analytics_logger.log_event('PARALLEL_LANGUAGE_DETECTION_COMPLETE',
                                      f'Detected languages: {detected_languages} (confidence: {avg_confidence:.2f})')

        return detected_languages

    def _detect_languages_sequential(self):
        """Sequential language detection for fallback"""
        languages = []

        # Check for various language indicators
        if (self.project_dir / "gofuzz").exists() or list(self.project_dir.glob("fuzz_*.go")):
            languages.append("go")

        if (self.project_dir / "fuzzers").exists() or list(self.project_dir.glob("fuzz_*.js")):
            languages.append("javascript")

        if (self.project_dir / "java").exists() or list(self.project_dir.glob("*Fuzz*.java")):
            languages.append("java")

        if list(self.project_dir.glob("fuzz_*.cpp")) or list(self.project_dir.glob("fuzz_*.cc")):
            languages.append("cpp")

        if list(self.project_dir.glob("fuzz_*.c")):
            languages.append("c")

        if (self.project_dir / "rust").exists() or list(self.project_dir.glob("fuzz_*.rs")):
            languages.append("rust")

        if list(self.project_dir.glob("fuzz_*.py")):
            languages.append("python")

        return languages

    def _run_intelligent_analysis(self):
        """Run pattern-based intelligent analysis and optimization"""
        self.analytics_logger.log_event('INTELLIGENT_ANALYSIS_START', 'Starting pattern-based analysis')

        # Intelligent language detection
        if self.config['enable_intelligent_detection']:
            print("ðŸ” Running intelligent language detection...")
            languages = self.detect_languages()
            print(f"ðŸ§  Detected languages: {languages}")

            # Analyze source files for intelligent insights
            source_files = []
            for ext in ['*.go', '*.js', '*.java', '*.cpp', '*.cc', '*.c', '*.rs', '*.py']:
                source_files.extend(list(self.project_dir.glob(f"**/{ext}")))

            # Analyze fuzz target potential
            if source_files:
                print("ðŸŽ¯ Analyzing fuzz target potential...")
                total_files = len(source_files)
                potential_targets = 0

                for i, file_path in enumerate(source_files[:50]):  # Analyze first 50 files
                    analysis = self.intelligent_detector.analyze_fuzz_target_potential(file_path)
                    if analysis['probability'] > self.config['fuzz_target_probability_threshold']:
                        potential_targets += 1

                self.intelligence_metrics.fuzz_target_probability = potential_targets / min(50, total_files)
                print(f"ðŸŽ¯ Potential fuzz targets detected: {potential_targets}/{min(50, total_files)} files")

        # Auto-optimization
        if self.config['enable_auto_optimization']:
            print("âš¡ Running auto-optimization analysis...")

            # Get current resource usage for optimization
            current_metrics = {
                'cpu_percent': self.analytics_logger.track_resource_usage().get('cpu_percent', 0),
                'memory_percent': self.analytics_logger.track_resource_usage().get('memory_percent', 0),
                'processing_rate_mbps': self.analytics_logger.performance_metrics.processing_rate_mbps
            }

            # Optimize parameters
            optimized_params = self.auto_optimizer.optimize_parameters(current_metrics)

            # Apply optimizations
            if optimized_params.max_workers != self.config['max_workers']:
                old_workers = self.config['max_workers']
                self.config['max_workers'] = optimized_params.max_workers
                # Reinitialize parallel processor with new worker count
                self.parallel_processor.shutdown(wait=True)
                self.parallel_processor = ParallelProcessor(
                    max_workers=self.config['max_workers'],
                    analytics_logger=self.analytics_logger
                )
                print(f"âš¡ Auto-optimized workers: {old_workers} â†’ {self.config['max_workers']}")

            print(f"âš¡ Optimization complete - Cycle #{self.auto_optimizer.optimization_cycles}")

        # Smart dictionary generation
        if self.config['enable_smart_dictionary']:
            print("ðŸ“š Generating smart dictionary...")
            languages = self.detect_languages()
            source_files = []
            for ext in ['*.go', '*.js', '*.java', '*.cpp', '*.cc', '*.c', '*.rs', '*.py']:
                source_files.extend(list(self.project_dir.glob(f"**/{ext}")))

            if source_files:
                smart_dict = self.smart_dictionary_generator.generate_smart_dictionary(source_files[:100], languages)
                self.intelligence_metrics.dictionary_relevance = min(len(smart_dict) / 100.0, 1.0)
                print(f"ðŸ“š Generated {len(smart_dict)} intelligent dictionary entries")

                # Save smart dictionary
                dict_path = self.project_dir / "smart_dictionary.txt"
                with open(dict_path, 'w') as f:
                    f.write('\n'.join(smart_dict))
                print(f"ðŸ“š Smart dictionary saved to: {dict_path}")

        # Update intelligence metrics
        self.intelligence_metrics.optimization_score = self._calculate_optimization_score()
        self.analytics_logger.log_event('INTELLIGENT_ANALYSIS_COMPLETE', 'Pattern-based analysis completed')

    def _calculate_optimization_score(self) -> float:
        """Calculate overall optimization score based on intelligence metrics"""
        language_score = self.intelligence_metrics.language_confidence
        target_score = self.intelligence_metrics.fuzz_target_probability
        dictionary_score = self.intelligence_metrics.dictionary_relevance

        # Weighted average of intelligence metrics
        optimization_score = (
            language_score * 0.4 +
            target_score * 0.4 +
            dictionary_score * 0.2
        )

        return min(optimization_score, 1.0)

    def cleanup(self):
        """Clean up resources and shutdown parallel processor"""
        if hasattr(self, 'parallel_processor'):
            self.parallel_processor.shutdown(wait=True)
            self.analytics_logger.log_event('CLEANUP', 'Parallel processor shutdown complete')

    def __del__(self):
        """Destructor to ensure cleanup"""
        try:
            self.cleanup()
        except:
            pass

    def generate_recommendations(self, results):
        """Generate recommendations based on results and intelligent analysis"""
        recommendations = []

        # Basic recommendations
        if results["fuzz_targets"] == 0:
            recommendations.append("No fuzz targets found. Check build script configuration.")

        if results["seed_corpora"] == 0:
            recommendations.append("No seed corpora found. Add seed directories for better fuzzing.")

        if results["dictionaries"] == 0:
            recommendations.append("No dictionaries found. Consider adding domain-specific dictionaries.")

        if results["fuzz_targets"] < 5:
            recommendations.append("Consider adding more fuzz targets to increase coverage.")

        if results["total_size_mb"] > 100:
            recommendations.append("Large output size detected. Consider optimizing build artifacts.")

                    # Intelligent recommendations based on pattern analysis
        if self.config['enable_auto_optimization']:
            intelligent_recommendations = self.auto_optimizer.generate_intelligent_recommendations(
                self.analytics_logger.expansion_stats
            )
            recommendations.extend(intelligent_recommendations)

        # Intelligence-based recommendations
        if self.intelligence_metrics.fuzz_target_probability < 0.5:
            recommendations.append("Low fuzz target potential detected - review source code for fuzzing opportunities.")

        if self.intelligence_metrics.optimization_score < 0.7:
            recommendations.append("Optimization opportunities detected - consider running with auto-optimization enabled.")

        return recommendations

    def run_full_expansion(self):
        """Run the complete rapid expansion process with full analytics"""
        print(f"ðŸš€ Starting **ULTIMATE** Rapid Fuzzer Expansion for {self.project_name}")
        print(f"OSS-Fuzz root: {self.oss_fuzz_root}")
        print(f"Project directory: {self.project_dir}")
        print(f"Analytics enabled: {self.config['analytics_enabled']}")
        print(f"Parallel processing: {self.config['enable_parallel_processing']} (max workers: {self.config['max_workers']})")
        print("=" * 80)

        try:
            # Start performance tracking
            if self.config['analytics_enabled']:
                self.analytics_logger.start_performance_tracking()
                self.analytics_logger.log_event('EXPANSION_START', 'Ultimate expansion process initiated')

            # Phase 0: Intelligent Analysis & Optimization
            if self.config['enable_intelligent_detection'] or self.config['enable_auto_optimization']:
                print("ðŸ§  Phase 0: Pattern-Based Analysis & Optimization")
                self._run_intelligent_analysis()

            # Phase 1: Environment Setup
            print("ðŸ“‹ Phase 1: Environment Setup")
            env_vars = self.setup_environment()

            # Phase 2: Validation
            print("\nðŸ“‹ Phase 2: System Validation")
            helper_ok = self.python_helper_check()

            # Phase 3: Build Execution
            print("\nðŸ“‹ Phase 3: Build Execution")
            build_ok = self.enhanced_build_script()
            self.analytics_logger.expansion_stats.build_success = build_ok

            # Phase 4: Analysis & Reporting
            print("\nðŸ“‹ Phase 4: Analysis & Reporting")
            results = self.analyze_results()

            # Create enhanced summary report with intelligent recommendations
            report = self.create_enhanced_report(results)

            # Stop performance tracking and generate analytics
            analytics_report = {}
            if self.config['analytics_enabled']:
                self.analytics_logger.stop_performance_tracking()
                analytics_report = self.analytics_logger.generate_analytics_report()

                # Log final performance metrics
                duration = self.analytics_logger.performance_metrics.duration_seconds
                processing_rate = self.analytics_logger.performance_metrics.processing_rate_mbps
                self.analytics_logger.log_event('EXPANSION_PERFORMANCE',
                                              f'Final performance: {duration:.2f}s duration, {processing_rate:.2f} MB/s processing rate')

            # Phase 4: Generate Professional Reports
            if any([self.config['enable_html_reports'], self.config['enable_pdf_reports'], self.config['enable_json_reports']]):
                print("\nðŸ“Š Phase 4: Generating Professional Reports")
                try:
                    reports = self.report_generator.generate_comprehensive_report(
                        results, analytics_report if analytics_report else {}, report['enhanced_recommendations']
                    )

                    if reports:
                        print("ðŸ“‹ Generated Reports:")
                        for report_type, report_path in reports.items():
                            print(f"  â€¢ {report_type.upper()}: {report_path}")

                        # Open HTML report automatically if possible
                        if 'html' in reports and self.config['enable_html_reports']:
                            html_path = reports['html']
                            print(f"ðŸŒ HTML Report: file://{html_path}")
                            try:
                                import webbrowser
                                webbrowser.open(f"file://{html_path}")
                                print("ðŸŒ HTML report opened in browser automatically")
                            except Exception:
                                pass  # Silently continue if browser opening fails

                except Exception as e:
                    print(f"âš ï¸  Report generation failed: {e}")

            # Final status with comprehensive feedback
            self.display_final_status(build_ok, report, analytics_report)

            return report

        except Exception as e:
            error_msg = f"Critical error during expansion: {e}"
            self.analytics_logger.expansion_stats.errors_encountered.append(error_msg)
            self.analytics_logger.log_event('EXPANSION_FAILED', error_msg, error=str(e))
            print(f"âŒ {error_msg}")
            sys.exit(1)
        finally:
            # Always cleanup resources
            self.cleanup()

    def create_enhanced_report(self, results):
        """Create comprehensive enhanced report"""
        report = self.create_summary_report(results)

        # Add analytics data if available
        if self.config['analytics_enabled']:
            analytics_data = self.analytics_logger.generate_analytics_report()
            report['analytics'] = analytics_data
            report['performance_summary'] = analytics_data['summary']

        # Enhanced recommendations based on analytics
        enhanced_recommendations = self.generate_enhanced_recommendations(results)
        report['enhanced_recommendations'] = enhanced_recommendations

        return report

    def generate_enhanced_recommendations(self, results):
        """Generate enhanced recommendations based on analytics"""
        recommendations = self.generate_recommendations(results)

        # Add performance-based recommendations
        if self.config['analytics_enabled']:
            metrics = self.analytics_logger.performance_metrics

            if metrics.processing_rate_mbps < 10:
                recommendations.append("Consider enabling parallel processing for better performance")

            if metrics.success_rate < 80:
                recommendations.append("High failure rate detected - review build dependencies")

            if self.analytics_logger.expansion_stats.total_size_mb > 500:
                recommendations.append("Large output size - consider implementing build artifact optimization")

        return recommendations

    def display_final_status(self, build_ok, report, analytics_report=None):
        """Display comprehensive final status"""
        print("\n" + "=" * 80)

        if build_ok:
            print("ðŸŽ‰ **ULTIMATE EXPANSION COMPLETED SUCCESSFULLY!** ðŸŽ‰")
            print(f"âœ… Build successful: {self.analytics_logger.expansion_stats.build_success}")
            print(f"âœ… Fuzz targets: {report['results']['fuzz_targets']}")
            print(f"âœ… Seed corpora: {report['results']['seed_corpora']}")
            print(f"âœ… Total size: {report['results']['total_size_mb']:.2f} MB")

            if analytics_report:
                summary = analytics_report['summary']
                print(f"âš¡ Performance: {summary['duration_seconds']:.2f}s duration")
                print(f"âš¡ Processing rate: {summary['processing_rate_mbps']:.2f} MB/s")
                print(f"âš¡ Success rate: {summary['success_rate_percent']:.1f}%")

            if report.get('enhanced_recommendations'):
                print("\nðŸ’¡ **Enhanced Recommendations:**")
                for i, rec in enumerate(report['enhanced_recommendations'], 1):
                    print(f"  {i}. {rec}")

        else:
            print("âŒ **EXPANSION FAILED!** âŒ")
            print("Build process encountered errors")

        print("=" * 80)

        # Save comprehensive analytics report
        if analytics_report and self.config['analytics_enabled']:
            analytics_file = self.project_dir / "ultimate_analytics_report.json"
            with open(analytics_file, 'w') as f:
                json.dump(analytics_report, f, indent=2, default=str)
            print(f"ðŸ“Š Analytics report saved to: {analytics_file}")

        print("ðŸ› Happy fuzzing! ðŸ›âœ¨")

def setup_source_directory(args):
    """Setup source directory, handling repository cloning if needed"""
    if args.repository:
        import tempfile
        import shutil
        import subprocess

        # Create temporary directory for cloning
        temp_base = Path(args.temp_dir) if args.temp_dir else Path(tempfile.gettempdir())
        temp_base.mkdir(exist_ok=True)

        # Create unique directory for this repository
        repo_name = args.repository.split('/')[-1].replace('.git', '')
        temp_dir = temp_base / f"rapid_expand_{repo_name}_{int(time.time())}"
        temp_dir.mkdir(exist_ok=True)

        print(f"ðŸ”„ Cloning repository: {args.repository}")
        print(f"ðŸ“ Target directory: {temp_dir}")

        try:
            # Clone the repository using system git command
            clone_cmd = ["git", "clone", args.repository, str(temp_dir)]
            result = subprocess.run(clone_cmd, capture_output=True, text=True, check=True)
            print(f"âœ… Repository cloned successfully")

            # Change to the cloned directory and checkout branch if specified
            if args.branch != "main":
                checkout_cmd = ["git", "checkout", args.branch]
                result = subprocess.run(checkout_cmd, cwd=temp_dir, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"âœ… Checked out branch: {args.branch}")
                else:
                    print(f"âš ï¸  Branch {args.branch} not found, using default branch")

            print(f"âœ… Repository ready at: {temp_dir}")
            return temp_dir, temp_dir  # cleanup_dir is temp_dir

        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to clone repository: {e}")
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error during repository setup: {e}")
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            sys.exit(1)
    else:
        # Use provided source directory or current working directory
        source_dir = Path(args.source_dir) if args.source_dir else Path.cwd()
        if not source_dir.exists():
            print(f"âŒ Source directory not found: {source_dir}")
            sys.exit(1)
        return source_dir, None

def main():
    parser = argparse.ArgumentParser(
        description="ðŸš€ **ULTIMATE** Rapid Fuzzer Expansion Tool - The most advanced fuzzing expansion system ever created!",
        formatter_class=argparse.RawDescriptionHelpFormatter,
                epilog="""
Examples:
  python3 rapid_expand.py                                    # Basic run for gemini_cli
  python3 rapid_expand.py --project my_project              # Custom project
  python3 rapid_expand.py --disable-analytics               # Disable analytics
  python3 rapid_expand.py --max-workers 8                   # 8 parallel workers
  python3 rapid_expand.py --enable-monitoring               # Extra resource monitoring

Repository Examples:
  python3 rapid_expand.py --repository https://github.com/google/gemini-cli.git
  python3 rapid_expand.py --repository https://github.com/user/project.git --branch develop
  python3 rapid_expand.py --repository git@gitlab.com:user/project.git --temp-dir /tmp/repos
        """
    )

    parser.add_argument("--project", default="gemini_cli", help="Project name (default: gemini_cli)")
    parser.add_argument("--oss-fuzz-root", help="OSS-Fuzz root directory")

    # Phase 5: Plugin Architecture options
    parser.add_argument("--source-dir", help="Source code directory to analyze")
    parser.add_argument("--output-dir", help="Output directory for generated artifacts")
    parser.add_argument("--plugins-dir", help="Custom plugins directory")
    parser.add_argument("--repository", help="Git repository URL to clone and analyze")
    parser.add_argument("--branch", default="main", help="Branch to checkout (default: main)")
    parser.add_argument("--temp-dir", help="Temporary directory for cloning repositories")
    parser.add_argument("--list-plugins", action="store_true", help="List all available plugins")
    parser.add_argument("--use-core-engine", action="store_true", help="Use the new plugin-based core engine")
    parser.add_argument("--generate-fuzzers", action="store_true", help="Generate fuzzing harnesses for discovered functions")

    # Analytics and monitoring options
    parser.add_argument("--disable-analytics", action="store_true",
                       help="Disable comprehensive analytics and performance tracking")
    parser.add_argument("--enable-monitoring", action="store_true",
                       help="Enable enhanced resource monitoring")
    parser.add_argument("--disable-progress", action="store_true",
                       help="Disable progress bars")

    # Performance options
    parser.add_argument("--max-workers", type=int, default=None,
                       help="Maximum number of parallel workers")
    parser.add_argument("--disable-parallel", action="store_true",
                       help="Disable parallel processing")

    # Output options
    parser.add_argument("--log-level", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help="Logging level (default: INFO)")

    # Phase 3: Intelligent Automation options
    parser.add_argument("--disable-intelligent-detection", action="store_true",
                       help="Disable pattern-based language and format detection")
    parser.add_argument("--disable-auto-optimization", action="store_true",
                       help="Disable auto-optimization and parameter tuning")
    parser.add_argument("--disable-smart-dictionary", action="store_true",
                       help="Disable intelligent dictionary generation")
    parser.add_argument("--language-confidence-threshold", type=float, default=0.7,
                       help="Minimum confidence threshold for language detection (default: 0.7)")
    parser.add_argument("--fuzz-target-threshold", type=float, default=0.6,
                       help="Minimum probability threshold for fuzz target detection (default: 0.6)")

    # Phase 4: Professional Reporting options
    parser.add_argument("--disable-html-reports", action="store_true",
                       help="Disable HTML report generation")
    parser.add_argument("--disable-pdf-reports", action="store_true",
                       help="Disable PDF report generation")
    parser.add_argument("--disable-json-reports", action="store_true",
                       help="Disable JSON analytics export")
    parser.add_argument("--disable-visualizations", action="store_true",
                       help="Disable chart and visualization generation")

    args = parser.parse_args()

    # Phase 5: Plugin-based Architecture
    if args.use_core_engine or args.list_plugins:
        # Initialize plugin manager
        plugins_dir = Path(args.plugins_dir) if args.plugins_dir else None
        plugin_manager = PluginManager(plugins_dir)

        if args.list_plugins:
            print("ðŸ”Œ Available Plugins:")
            plugins = plugin_manager.list_plugins()
            for plugin_type, plugin_list in plugins.items():
                print(f"  {plugin_type.title()}: {', '.join(plugin_list) if plugin_list else 'None'}")
            return

        # Setup directories and handle repository cloning
        source_dir, cleanup_dir = setup_source_directory(args)
        output_dir = Path(args.output_dir) if args.output_dir else source_dir / "fuzzing_output"
        output_dir.mkdir(exist_ok=True)

        # Create config
        config = {
            'project_dir': str(source_dir),
            'max_workers': args.max_workers or 4,
            'enable_parallel_processing': not args.disable_parallel,
            'analytics_enabled': not args.disable_analytics,
            'enable_resource_monitoring': args.enable_monitoring,
            'enable_intelligent_detection': not args.disable_intelligent_detection,
            'enable_auto_optimization': not args.disable_auto_optimization,
            'enable_smart_dictionary': not args.disable_smart_dictionary,
            'enable_html_reports': not args.disable_html_reports,
            'enable_pdf_reports': not args.disable_pdf_reports,
            'enable_json_reports': not args.disable_json_reports,
            'generate_fuzzers': args.generate_fuzzers
        }

        # Initialize and run core engine
        core_engine = CoreEngine(plugin_manager, config)
        results = core_engine.run_expansion(source_dir, output_dir)

        if results['success']:
            print("ðŸŽ‰ Plugin-based expansion completed successfully!")
            if 'reports' in results and results['reports']:
                print("ðŸ“Š Generated reports:")
                for format_name, report_path in results['reports'].items():
                    print(f"  â€¢ {format_name.upper()}: {report_path}")
        else:
            print(f"âŒ Expansion failed: {results.get('error', 'Unknown error')}")
            sys.exit(1)

        # Cleanup temporary directory if repository was cloned
        if cleanup_dir and cleanup_dir.exists():
            import shutil
            try:
                shutil.rmtree(cleanup_dir)
                print(f"ðŸ§¹ Cleaned up temporary directory: {cleanup_dir}")
            except Exception as e:
                print(f"âš ï¸  Could not cleanup temporary directory: {e}")

        return

    # Original monolithic architecture (for backward compatibility)
    expander = RapidExpander(args.project, args.oss_fuzz_root)

    # Apply configuration overrides
    if args.disable_analytics:
        expander.config['analytics_enabled'] = False

    if args.enable_monitoring:
        expander.config['enable_resource_monitoring'] = True

    if args.disable_progress:
        expander.config['progress_bar_enabled'] = False

    if args.disable_parallel:
        expander.config['enable_parallel_processing'] = False

    if args.max_workers:
        expander.config['max_workers'] = args.max_workers

    expander.config['log_level'] = args.log_level

    # Apply Phase 3 intelligent automation configurations
    if args.disable_intelligent_detection:
        expander.config['enable_intelligent_detection'] = False

    if args.disable_auto_optimization:
        expander.config['enable_auto_optimization'] = False

    if args.disable_smart_dictionary:
        expander.config['enable_smart_dictionary'] = False

    expander.config['language_confidence_threshold'] = args.language_confidence_threshold
    expander.config['fuzz_target_probability_threshold'] = args.fuzz_target_threshold

    # Apply Phase 4 professional reporting configurations
    if args.disable_html_reports:
        expander.config['enable_html_reports'] = False

    if args.disable_pdf_reports:
        expander.config['enable_pdf_reports'] = False

    if args.disable_json_reports:
        expander.config['enable_json_reports'] = False

    if args.disable_visualizations:
        expander.config['enable_visualizations'] = False

    # Display configuration
    print(f"ðŸ”§ Configuration:")
    for key, value in expander.config.items():
        print(f"  {key}: {value}")
    print()

    # Run the ultimate expansion
    expander.run_full_expansion()

if __name__ == "__main__":
    main()

